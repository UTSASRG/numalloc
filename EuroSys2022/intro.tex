

%\todo{We need to add the memory difference with and without the transparent memory.} 

\section{Introduction}
\label{sec:intro}

The Non-Uniform Memory Access (NUMA) architecture is a scalable hardware design. Compared to Uniform Memory Access (UMA) architecture, the NUMA architecture avoids the bottleneck of using one memory controller, where each processor (or node interchangeably) can access its own memory controller concurrently in theory. However, it is extremely challenging to achieve the expected scalability for multithreaded applications. One notorious issue is caused by remote accesses that a task accesses the memory located in a remote node, which hurts the application performance since the latency of remote accesses is much higher than that of local accesses~\cite{Blagodurov:2011:CNC:2002181.2002182}. In addition to that, node imbalance may actually introduce the congestion of memory controllers or interconnection ~\cite{Blagodurov:2011:CNC:2002181.2002182}. Therefore, it is critical to reducing remote accesses or node imbalance for multithreaded applications. Although programmers could employ the assistance of different profiling tools to fix NUMA performance issues within applications~\cite{Intel:VTune, Memphis, Lachaize:2012:MMP:2342821.2342826, XuNuma, NumaMMA, 7847070, NumaPerf}, they cannot fix NUMA performance issues caused by a memory allocator.
%For instance, the latency of remote accesses is typically double to that of local accesses. 

However, general-purpose memory allocators, such as dlmalloc~\cite{dlmalloc},  Hoard~\cite{Hoard}, TCMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, SuperMalloc~\cite{supermalloc}, and  Scalloc~\cite{Scalloc}, which were designed for symmetric multiprocessing machines. As a result, they cannot achieve good performance for NUMA architecture, based on existing work~\cite{tcmallocnew, yang2019jarena} and our evaluation. 

There exist some NUMA-aware allocators~\cite{tcmallocnew, kim2013node, yang2019jarena, mimalloc}. In particular, Kaminski built the first NUMA-aware memory allocator on top of TCMalloc in 2008~\cite{tcmallocnew}, called \TN{} in the remainder of this paper. \TN{} adds a freelist array and a page-span for each NUMA node, where allocations are satisfied in the order of the per-thread cache, the per-node freelist, and then the per-node page-span. To reduce remote access, \TN{} allocates the physical memory of each page-span from the same node as the current thread. mimalloc proposes a page-based freelist that could only serve a thread at a time~\cite{mimalloc}, where all objects will be returned to the same page-based freelist upon deallocations. By allocating physical memory of each page locally, mimalloc also achieves some level of the locality. However, \textit{none of these allocators achieve the full locality of memory allocations}. Both \TN{} and mimalloc may introduce unnecessary remote accesses, if a thread is scheduled to a remote node. For \TN{}, a freed object is always placed into the deallocating thread's local buffer, which will violate the locality if this object is originally allocated from a remote node  (e.g., in the producer-consumer model). 
%Similarly, mimalloc will also introduce remote accesses when a thread is migrated to a different node. 
Further, none of these allocators balance the workload between nodes, another source of performance issue~\cite{Dashti:2013:TMH:2451116.2451157}. 

%remote node upon synchronizations or system calls, which not only forces the thread to access its stack remotely but also reloads all the data that are already in the cache of its original node. Further, after migration, all deallocated objects of the thread are later put into the freelist of the new node, causing unnecessary remote accesses. To the best of our knowledge, none of the existing NUMA allocators address these issues well.


\textit{This paper proposes a novel memory allocator -- \NM{} -- to address these issues}.  \NM{} not only ensures the full locality of memory allocations, but also considers load balance within its design. It further proposes a fine-grained memory management that employs different policies for objects with different attributes, such as share-ability, phase, and origin. Its mechanisms are detailed in the following.   

%Simply integrating thread binding, heap interleaving and huge page does not guarantee good performance. Instead, carefully design is required to properly coordinate the thread scheduling, memory object allocation/deallocation and huge page management to achieve better performance.

A \textbf{origin-based memory management} is proposed to ensure that memory allocations are always satisfied from the local node of the requesting thread. Note that this is significantly different from the first-touch policy~\cite{lameter2013numa} that allocates a physical page from the node with the first touch, since the first-touch policy does not handle deallocations of objects. Instead, the origin-based memory management will track the origin of each object, and return a freed object based on its origin: a freed object will be placed into the deallocating thread's local buffer (heap) \textbf{only if} it is originated from the same node that the current thread is running on; otherwise, it will be returned to its original node. \textit{Second}, \NM{} introduces an origin-computable design that the physical origin can be simply computed within constant time, as further described in Section~\ref{sec:overview}. This design helps reduce the overhead of checking the origin of each object upon deallocations. \textit{Third}, it also includes an origin-based allocation that will perform the allocations based on the origin of the thread, where each thread is pinned to a specific node as described below. These mechanisms altogether ensure that all objects allocated by a thread will be always originated (pysically) from the same node as the thread. 

Further, two mechanisms are proposed to reduce load imbalance among different NUMA nodes. The first mechanism is a \textbf{node-balanced thread binding} that reduces remote accesses and loads imbalance concurrently. We argue that the thread binding should be the \textit{first-class citizen} for the NUMA memory allocator, as the cross-node migration of a thread will basically turn all of its previously local accesses into remote ones, and confuse its consequent memory allocations and deallocations. \NM{} is the first memory allocator that embeds the node-balanced thread binding inside, although the thread-binding idea is not new. \NM{} binds threads to nodes in a round-robin way so that each node will have a similar number of threads, helping reduce the congestion of a particular memory controller~\cite{Blagodurov:2011:CNC:2002181.2002182}. We evaluated that this node-balanced thread binding will also benefit two other allocators.  The second mechanism is \textbf{an interleaved heap} for potentially-shared objects from the main thread, where physical pages of this heap are allocated from different physical nodes in an interleaved way. This is inspired by existing profilers that shared objects allocated from the initial/main thread are the most common source of performance degradation in the NUMA architecture~\cite{XULIU, MemProf}. With the default first-touch policy, such objects will be allocated physically from the node of the main thread. But they can be accessed by multiple children threads (possibly running on different nodes) concurrently, making this node the performance bottleneck. Instead, \NM{} allocates potentially-shared objects of the main thread  from all nodes evenly,  which helps distribute concurrent accesses from children threads to all nodes. Note that the interleaved heap my introduce unnecessary remote accesses for the initial thread in the serial phase. That is, the interleaved heap should be exploited cautiously. Luckily, it is easy to determine that the interleaved heap is not a good option for applications spending a high percentage of time in the serial phase.    
%Note that objects from the interleaved heap will be always tracked in its own corresponding freelists. , which reduces load imbalance for the initial node that the main thread is running on
%Due to the binding, \NM{} can  allocate the metadata of each thread (e.g., per-thread buffer) from the local node, which cannot be done without the binding. 

%in order to reduce load imbalance or interconnect congestion that occurs when a large amount of memory allocated in the initial thread are accessed extensively by children threads running on different nodes. 
%second, it may cause page-level false sharing, if a multi-page object is accessed concurrently by these threads, the most common NUMA performance detected by existing tools~\cite{XULIU, MemProf}. The underlying reason is that the first-touch policy makes objects allocated from the initial thread will be allocated from its corresponding node, as further described in Section~\ref{sec:ossupport}. The interleaved heap helps node balance, and alleviates page-level false sharing. 
 
%\NM{} further introduces \textbf{selective huge pages} in order to exploit the benefit of huge pages. Huge pages will improve the performance if they are employed properly~\cite{DBLP:conf/asplos/MaasAIJMR20}, due to the reduction of TLB misses. However, the default transparent huge pages (THP) have been observed to hurt the performance on the NUMA architecture, due to unnecessary memory wastes and page-level false sharing~\cite{Gaud:2014:LPM:2643634.2643659, DBLP:conf/asplos/PanwarBG19, DBLP:conf/asplos/MaasAIJMR20}. \textit{\NM{} controls the type of objects that will be allocated from huge pages, in order to avoid the issues caused by huge pages}. In \NM{}, huge pages will be utilized only for big objects, as well as small objects with extensive allocations. This design is to reduce memory wastes. Based on our evaluation in Section~\ref{sec:hugepage}, \NM{} selective huge pages do not hurt the performance, compared to transparent huge pages.
 
\NM{} also has some implementation novelty. 
%It designs  has many per-thread and per-node heaps, it is necessary to move freed objects between per-thread freelists and per-node freelists frequently. 
It designs an efficient mechanism that could move objects between different freelists, without traversing all objects in the freelists as TCMalloc~\cite{tcmalloc}. 
%Due to the binding, the metadata of per-thread and per-node heap on the local nodes in order to further reduce unnecessary remote accesses.  
It also reduces memory consumption of transparent huge pages by making multiple threads share the same memory block, overcoming the significant memory overhead like Scalloc~\cite{Scalloc}. 

We have performed extensive evaluation on synthetic and real applications, and compared \NM{} with popular allocators, such as the default Linux allocator, TCMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, Intel TBB~\cite{tbb}, Scalloc~\cite{Scalloc}, and mimalloc~\cite{mimalloc}. \NM{} achieves around 18.5\%  (GEOMEAN) speedup comparing to the default Linux allocator, which is also 15.8\% faster than the second-best one (e.g., TCMalloc). For the best case, \NM{} runs up to $6.4\times$ faster than the default allocator, and $4.6\times$ faster than the second-best one (TCMalloc). \NM{} is much more scalable than other allocators based on our evaluation. \NM{} is ready for practical employment, due to its high performance and good scalability. Overall, this paper makes the following contributions:

\begin{itemize}

\item It proposes a \textbf{fine-grained memory management} method that will consider both hardware heterogeneity and different allocation/deallocation attributes in memory management operations.

\item It proposes an \textbf{origin-based memory management} to ensure locality of all memory allocations. It also proposes \textbf{node-balanced thread binding and the interleaved heap} to reduce node imbalance. 

\item It presents the development and implementation of \NM{}, which has a better performance and scalability than widely-used commercial allocators, such as TCMalloc, jemalloc, and Intel TBB, based on our extensive evaluation.  

\end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:background} introduces the OS support for the NUMA architecture and common designs of memory allocators. Section~\ref{sec:implement} focuses on the design and implementation of \NM{}. After that, Section~\ref{sec:evaluation} describes its experimental evaluation, and Section~\ref{sec:limit} discusses the limitation of \NM{}. In the end, Section~\ref{sec:related} discusses some relevant work, and then Section~\ref{sec:conclusion} concludes. 