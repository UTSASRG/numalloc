

%\todo{We need to add the memory difference with and without the transparent memory.} 
\begin{comment}
\todo{
1. verify the impact with and without origin-based deallocation (need to change few lines of code).

2. TCMalloc-version. TBB version. 

3. support the change of configurations via environment variables (binding, interleave heap). BTW, we don't need to check the size of objects any more. 

4. Memory overhead (mostly due to huge page, origin-based allocation) 
deallocations send back to the os when above some threadhold. 


Maybe we should target Micro-2022 instead. Then we should add the hardware organization figure. 

5. Let's claim our support of transparent page support as a novelty as well. 

6. check where the performance improvement is coming from, such as transparent pages, region-aware allocations, and interleaved heap.

7. If we don't ensure locality of deallocations, what is the performance impact. 
}

\end{comment}

\section{Introduction}
\label{sec:intro}

%The Non-Uniform Memory Access (NUMA) architecture is a scalable hardware design. Compared to Uniform Memory Access (UMA) architecture, the NUMA architecture avoids the bottleneck of using one memory controller, where each processor (or node interchangeably) can access its own memory controller concurrently in theory. However, it is extremely challenging to achieve the expected scalability for multithreaded applications. One notorious issue is caused by remote accesses that a task accesses the memory located in a remote node, which hurts the application performance since the latency of remote accesses is much higher than that of local accesses~\cite{Blagodurov:2011:CNC:2002181.2002182}. In addition to that, node imbalance may actually introduce the congestion of memory controllers or interconnection ~\cite{Blagodurov:2011:CNC:2002181.2002182}. Therefore, it is critical to reducing remote accesses or node imbalance for multithreaded applications. Although programmers could employ the assistance of different profiling tools to fix NUMA performance issues within applications~\cite{Intel:VTune, Memphis, Lachaize:2012:MMP:2342821.2342826, XuNuma, NumaMMA, 7847070, NumaPerf}, they cannot fix NUMA performance issues caused by a memory allocator.
%For instance, the latency of remote accesses is typically double to that of local accesses. 

\textbf{Motivation.} In Non-Uniform Memory Access (NUMA) architecture, each processor (or node interchangeably) has its own memory, allowing multiple nodes to access the memory concurrently, which is a scalable hardware design. 
However, it is extremely challenging to achieve the expected scalability for multithreaded applications. One notorious issue is caused by remote accesses that a task accesses the memory located in a remote node, which hurts the application performance since the latency of remote accesses is much higher than that of local accesses~\cite{Blagodurov:2011:CNC:2002181.2002182}. 
%In addition to that, node imbalance may actually introduce the congestion of memory controllers or interconnection ~\cite{Blagodurov:2011:CNC:2002181.2002182}. Therefore, it is critical to reducing remote accesses or node imbalance for multithreaded applications. 
Although programmers may employ profiling tools to identify NUMA issues of applications~\cite{Intel:VTune, Memphis, Lachaize:2012:MMP:2342821.2342826, XuNuma, NumaMMA, 7847070, NumaPerf}, they cannot fix the performance issues caused by a memory allocator.


\textbf{Limitation of state-of-art approaches.} General-purpose memory allocators, such as dlmalloc~\cite{dlmalloc},  Hoard~\cite{Hoard}, TCMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, SuperMalloc~\cite{supermalloc}, and  Scalloc~\cite{Scalloc},  were designed for symmetric multiprocessing machines. As a result, they cannot achieve good performance for NUMA architecture, based on existing work~\cite{tcmallocnew, yang2019jarena} and our evaluation. There exist some NUMA-aware allocators~\cite{ tcmallocnew, tcmalloc2, kim2013node, yang2019jarena, mimalloc}. In particular, Kaminski built the first NUMA-aware memory allocator on top of TCMalloc in 2008~\cite{tcmallocnew}, called \TN{} in the remainder of this paper. \TN{} adds a freelist array and a page-span for each NUMA node, where allocations are satisfied in the order of the per-thread cache, the per-node freelist, and then the per-node page-span. To reduce remote access, \TN{} allocates the physical memory of each page-span from the same node as the current thread. mimalloc proposes a page-based freelist that could only serve a thread at a time~\cite{mimalloc}, where all objects will be returned to the same page-based freelist upon deallocations. By allocating physical memory of each page locally, mimalloc also achieves some level of the locality. However, \textit{none of these allocators achieve the full locality of memory allocations}. Both \TN{} and mimalloc may introduce unnecessary remote accesses, if a thread is scheduled to a remote node. For \TN{}, a freed object is always placed into the deallocating thread's local buffer, which will violate the locality if this object is originally allocated from a remote node  (e.g., in the producer-consumer model). 
%Similarly, mimalloc will also introduce remote accesses when a thread is migrated to a different node. 
%Further, none of these allocators balance the workload between nodes, another source of performance issue~\cite{Dashti:2013:TMH:2451116.2451157}. 

%remote node upon synchronizations or system calls, which not only forces the thread to access its stack remotely but also reloads all the data that are already in the cache of its original node. Further, after migration, all deallocated objects of the thread are later put into the freelist of the new node, causing unnecessary remote accesses. To the best of our knowledge, none of the existing NUMA allocators address these issues well.


\textbf{Key insights and contributions.} This paper proposes a new memory allocator -- \NM{} -- to address these issues.  
%\NM{} not only ensures the locality of memory allocations, but also considers load balance within its design. 
%It further proposes a fine-grained memory management that employs different policies for objects with different attributes, such as share-ability, phase, and origin. Its mechanisms are detailed in the following.   
First, it proposes the first \textbf{binding-based memory management} to eliminate unnecessary remote accesses caused by thread migration. Although the ``thread binding'' has been frequently employed to improve the performance of applications on NUMA architecture~\cite{li2013numa, XuNuma, Lepers:2015:TMP:2813767.2813788}, it has never been the \textit{first-class citizen} for the memory allocator design. Compared to external binding, the inherent  binding inside the memory allocator provides some additional benefits: (1) its per-thread heap's metadata could be allocated locally, helping improve the locality; (2) It is easy to locate the location of threads without invoking expensive system calls, enabling \NM{} to build its origin-aware memory management as discussed below. Further, the thread binding eliminates the cross-node migration of threads, preventing them from turning their previously-local accesses into remote ones. \NM{} utilizes the node-interleaved thread binding  (binding threads to NUMA nodes interleavedly) by default, but it also supports node-saturate thread binding that will assign the same number of threads as the number of cores to a node before assigning threads to the next node. In the future, we plan to support 
user-defined thread binding via the environment variable or a configuration file. Based on our experiments on multiple allocators, the node-interleaved thread binding performs the best performance when all nodes are used to run one application. Note that \NM{}'s thread binding does not exclude the OS scheduling, as it only binds threads to the NUMA node, but not a specific core. Section~\ref{sec:limit} further discusses its design trade-off.  

Second, \NM{} propose the \textbf{origin-aware memory management} to ensure full memory locality. Memory locality is defined as whether an object is allocated from the local physical node of the requesting thread. Existing numa-aware allocators only ensure the locality of every object's first memory allocation~\cite{ tcmallocnew, kim2013node, yang2019jarena, mimalloc}.  Instead, \NM{} additionally ensures the locality of all freed objects, eliminating the confusion caused by memory reuse (a very common behavior). \NM{} guarantees that a freed object will not be placed into a freelist belonging to a remote node. In particular, \NM{}'s thread binding allows it to determine the thread's physical node, and \NM{}'s origin-based design can quickly determine each object's origin by the address's range. Then a freed object will be placed into the deallocating thread's local buffer (heap) \textit{only if} it is originated from the same node that the current thread is running on; otherwise, it will be returned back to its original node. 
%The thread binding allows \NM{} to check each node's origin quickly. Further,  the virtual address space of the heap is divided into multiple blocks, where each block is bound to a physical node, allowing to track each object's origin by the address's range. \NM{} also ensures the origin-based allocation that always allocate an object from the same node (physically) as the request thread, combining the above-mentioned mechanisms altogether to ensure the locality of allocations. 

Third, \NM{} also proposes \textbf{threads-shared incremental allocation} to embrace the default ``Transparent Huge Pages'' (THP) of modern OSes. Huge pages are expected to significantly reduce Translation Lookaside Buffer (TLB) misses, as each page table entry could cover a larger range of virtual addresses (e.g., 2MB instead of 4KB). However, on the one side, most existing allocators, such as dlmalloc~\cite{dlmalloc},  Hoard~\cite{Hoard}, TCMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, do not take the advantage of THP, as they typically map small chunks of memory (e.g., multiple kilobytes) each time that will be satisfied from small pages. On the other side, some allocators, such as Scalloc~\cite{Scalloc} and mimalloc~\cite{mimalloc}, consumes up to $6\times$ (as shown in Table~\ref{tab:memory_consumption}) more memory without a THP-friendly design. In contrast, \NM{} maps a large chunk of memory from each physical node, allowing its use of huge pages. Further, \NM{} only allocates a small chunk of memory to each requested thread each time, reducing the memory wastes if the thread does not need  much memory. Overall, \NM{} achieves a better balance between the performance and memory consumption. 

\NM{} also has some novel implementation. It designs an efficient mechanism that could move objects between different freelists, without traversing all objects in the freelist as TCMalloc~\cite{tcmalloc}. It also designs \textit{an interleaved heap} for allocating heap objects of the main thread from all nodes evenly, helping reduce the congestion caused by concurrent accesses from multiple children threads. This design is inspired by existing profilers that shared objects allocated from the initial/main thread are the most common source of performance degradation in the NUMA architecture~\cite{XuNuma, MemProf}. The interleaved heap will be a beneficial option for some applications, where users could enable based on the memory usage pattern of their applications. 

\textbf{Experimental methodology and artifact availability.} We have performed an extensive evaluation on synthetic and real applications, with 24 applications in total. Compared \NM{} with popular allocators, such as the default Linux allocator, TCMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, Intel TBB~\cite{tbb}, Scalloc~\cite{Scalloc}, and mimalloc~\cite{mimalloc},  \NM{} is running 17\% faster than the second-best allocator (mimalloc), achieving around 19\% speedup comparing to the default Linux allocator. For the best case, \NM{} runs up to $6.4\times$ faster than the default allocator. \NM{} is much more scalable than all other allocators based on our evaluation. \NM{} is ready for practical employment, due to its high performance and good scalability. The code will be opened source using GNU GPL V2 license at \url{https://github.com/XXX} (opened upon acceptance). 

\textbf{Limitations of the proposed approach.} The proposed work may need some user customization via configuration flags, such as interleaved heap or a specific format of thread binding, based on memory patterns of user applications. Some of these mechanisms, e.g. interleaved heap, are not universally beneficial to the performance of all applications. However, they do improve the performance of some applications. Therefore, we decide to keep such options, instead of discarding them. 

Overall, this paper makes the following contributions:

\begin{itemize}

\item It proposes a \textbf{binding-based memory management} to eliminate unnecessary remote accesses caused by thread migrations.

\item It proposes an \textbf{origin-aware memory management} to ensure full locality of memory allocations. 

\item It proposes a \textbf{threads-shared incremental allocation} to achieve a better balance between the performance and memory consumption of huge pages. 

\item It presents the development and implementation of \NM{}, which has a better performance and scalability than even widely-used commercial allocators.
%, such as TCMalloc, jemalloc, and Intel TBB, based on our extensive evaluation.  

\end{itemize}

%The remainder of this paper is organized as follows. Section~\ref{sec:background} introduces the OS support for the NUMA architecture and common designs of memory allocators. Section~\ref{sec:implement} focuses on the design and implementation of \NM{}. After that, Section~\ref{sec:evaluation} describes its experimental evaluation, and Section~\ref{sec:limit} discusses the limitation of \NM{}. In the end, Section~\ref{sec:related} discusses some relevant work, and then Section~\ref{sec:conclusion} concludes. 