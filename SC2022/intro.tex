

%\todo{We need to add the memory difference with and without the transparent memory.} 
\begin{comment}
\todo{
1. verify the impact with and without origin-based deallocation (need to change few lines of code).

2. TCMalloc-version. TBB version. 

3. support the change of configurations via environment variables (binding, interleave heap). BTW, we don't need to check the size of objects any more. 

4. Memory overhead (mostly due to huge page, origin-based allocation) 
deallocations send back to the os when above some threadhold. 


Maybe we should target Micro-2022 instead. Then we should add the hardware organization figure. 

5. Let's claim our support of transparent page support as a novelty as well. 

6. check where the performance improvement is coming from, such as transparent pages, region-aware allocations, and interleaved heap.

7. If we don't ensure locality of deallocations, what is the performance impact. 
}

\end{comment}

\section{Introduction}
\label{sec:intro}

%The Non-Uniform Memory Access (NUMA) architecture is a scalable hardware design. Compared to Uniform Memory Access (UMA) architecture, the NUMA architecture avoids the bottleneck of using one memory controller, where each processor (or node interchangeably) can access its own memory controller concurrently in theory. However, it is extremely challenging to achieve the expected scalability for multithreaded applications. One notorious issue is caused by remote accesses that a task accesses the memory located in a remote node, which hurts the application performance since the latency of remote accesses is much higher than that of local accesses~\cite{Blagodurov:2011:CNC:2002181.2002182}. In addition to that, node imbalance may actually introduce the congestion of memory controllers or interconnection ~\cite{Blagodurov:2011:CNC:2002181.2002182}. Therefore, it is critical to reducing remote accesses or node imbalance for multithreaded applications. Although programmers could employ the assistance of different profiling tools to fix NUMA performance issues within applications~\cite{Intel:VTune, Memphis, Lachaize:2012:MMP:2342821.2342826, XuNuma, NumaMMA, 7847070, NumaPerf}, they cannot fix NUMA performance issues caused by a memory allocator.
%For instance, the latency of remote accesses is typically double to that of local accesses. 
Non-Uniform Memory Access (NUMA) is the de-facto design for all modern hardware in order to address the scalability issues of increasing hardware cores. In NUMA architecture, each processor (or node interchangeably) has its own memory, allowing multiple nodes to access the memory concurrently. However, it is extremely challenging to achieve the expected scalability for multithreaded applications. One notorious issue is caused by remote accesses that a task accesses the memory located in a remote node, which hurts the application performance since the latency of remote accesses is much higher than that of local accesses~\cite{Blagodurov:2011:CNC:2002181.2002182}. 
%In addition to that, node imbalance may actually introduce the congestion of memory controllers or interconnection ~\cite{Blagodurov:2011:CNC:2002181.2002182}. Therefore, it is critical to reducing remote accesses or node imbalance for multithreaded applications. 
Although programmers may employ profiling tools to identify NUMA issues of applications~\cite{Intel:VTune, Memphis, Lachaize:2012:MMP:2342821.2342826, XuNuma, NumaMMA, 7847070, NumaPerf}, they cannot fix the performance issues caused by a memory allocator.

% Based on this guide https://libraryguides.vu.edu.au/ieeereferencing/gettingstarted#s-lg-box-wrapper-9930413, it's acceptable
% \todo{The IEEE seems to have multiple references separately but I am not used to it. Double check it.}

General-purpose memory allocators, such as dlmalloc~\cite{dlmalloc},  Hoard~\cite{Hoard}, TCMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, SuperMalloc~\cite{supermalloc}, and  Scalloc~\cite{Scalloc}, were designed for symmetric multiprocessing machines. As a result, they cannot achieve good performance for NUMA architecture, based on existing work~\cite{tcmallocnew, yang2019jarena} and our evaluation. There exist some NUMA-aware allocators~\cite{tcmallocnew, tcmalloc2, kim2013node, yang2019jarena, mimalloc}. In particular, Kaminski built the first NUMA-aware memory allocator on top of TCMalloc in 2008~\cite{tcmallocnew}, called \TN{} in the remainder of this paper. \TN{} adds a freelist  and a page-span for each NUMA node so that threads can allocate objects from its per-node list/page-span. To reduce remote access, \TN{} allocates the physical memory of each page-span from the same node as the current thread, where mimalloc~\cite{mimalloc} and recent NUMA support of TCMalloc follows the similar idea.  However, \textit{none of these allocators achieve the full locality of memory allocations}, as they did not handle remote access caused by memory deallocations: a freed object is typically placed into the deallocating thread's local buffer, which will violate the locality if this object is originally allocated from a remote node  (e.g., in the producer-consumer model). More importantly, they do not consider the performance impact caused by potential thread migration, and  cannot take the full advantage of huge pages that are prevalent in modern hardware. Instead, \NM{} overcomes these issues as follows. 

\text{First}, it proposes the first \textbf{binding-based memory management} to eliminate unnecessary remote accesses caused by thread migrations. A thread's migration will turn all its previously-local accesses into remote ones, and therefore can significantly degrade the performance. Although the ``thread binding'' has been frequently employed to improve the performance of applications on NUMA architecture~\cite{li2013numa, XuNuma, Lepers:2015:TMP:2813767.2813788}, it has never been the \textit{first-class citizen} for the memory allocator design. That is, existing work  binds threads of applications externally,  but the memory allocator is not aware of the binding. In contrast, the proposed work still allow users to customize the binding via the configuration file or environment variable, but \textit{will exploit the binding to improve its memory management} as follows: (1) all metadata can be allocated in the local node, based on the binding; (2) It can locate the location of threads without invoking expensive system calls, enabling full locality of memory management as discussed below. Note that \NM{}'s thread binding does not exclude the OS scheduling, as it only binds threads to the NUMA node, but not a specific core. Section~\ref{sec:limit} further discusses its design trade-off.  


%The thread binding allows \NM{} to check each node's origin quickly. Further,  the virtual address space of the heap is divided into multiple blocks, where each block is bound to a physical node, allowing to track each object's origin by the address's range. \NM{} also ensures the origin-based allocation that always allocate an object from the same node (physically) as the request thread, combining the above-mentioned mechanisms altogether to ensure the locality of allocations. 

Second, \NM{} proposes \textbf{threads-shared incremental allocation} to take advantage of ``Transparent Huge Pages'' (THP) of modern OS/hardware~\cite{hugepage}. Huge pages are expected to significantly reduce Translation Lookaside Buffer (TLB) misses, as each page table entry could cover a larger range of virtual addresses (e.g., 2MB instead of 4KB). However, most of the existing allocators~\cite{dlmalloc, Hoard, tcmalloc, mimalloc} could not take advantage of huge pages, where Scalloc even suggests that ``make sure that transparent huge pages are disabled'' due to its large memory overhead~\cite{scallochugepage}. TEMERAIRE~\cite{TEMERAIRE} and LLAMA~\cite{LLAMA} support huge pages with complicated mechanisms. Instead, the proposed work designs a simple mechanism for managing huge pages: it maps huge regions (with the size larger than a huge page) of memory initially so that these regions will be backed by huge pages physically inside the OS; Further, it utilizes the same region to serve allocations with different size classes of different threads so that each thread only allocates a small number of objects from a huge region, called as ``incremental allocation''. Incremental allocations actually reduces the memory wastes, comparing to Scalloc that always allocates a big chunk of memory to each requested thread. 
Overall, \NM{} combines the best of both worlds that it takes the performance advantage of huge pages but does not compromise its memory consumption. 
%a better balance between performance and memory consumption.

\todo{Both TEMERAIRE~\cite{TEMERAIRE} and LLAMA~\cite{LLAMA} support huge pages, but with complicated mechanisms. Instead, the proposed work designs a simple mechanism for managing huge pages: it maps a large region of memory for each node initially, and these regions will be backed by huge pages physically inside the OS. Further, it utilizes the same region to serve allocations with different size classes from different threads that running on the same node. In this way, each thread only gets a small number of objects from a huge region, called as ``incremental allocation''.}

%However, on the one side, most existing allocators, such as dlmalloc~\cite{dlmalloc}, Hoard~\cite{Hoard}, TCMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, do not take advantage of THP, as they typically map small chunks of memory (e.g., multiple kilobytes) each time that will be satisfied from small pages. On the other side, some allocators, such as Scalloc~\cite{Scalloc} and mimalloc~\cite{mimalloc}, consumes up to $7\times$ (as shown in Table~\ref{tab:memory_consumption}) more memory without a THP-friendly design. 
\NM{} also has other novelty designs. (1) It proposes the \textbf{origin-aware memory management} to ensure the full memory locality built on top of its binding-based mechanism: \NM{} guarantees that a freed object will be returned to the deallocating thread \textit{only if} the object is originated from the same node that the current thread is running on; otherwise, it will be returned back to its original node. (2) It designs \textbf{an interleaved heap} for allocating heap objects of the main thread from all nodes evenly, helping reduce the congestion caused by concurrent accesses from multiple children threads. This design is inspired by existing profilers' finding that shared objects allocated from the initial/main thread are the most common source of performance degradation in the NUMA architecture~\cite{XuNuma, MemProf}. The interleaved heap will be a beneficial option for some applications. (3) It designs an efficient mechanism that could move objects between different freelists, without traversing all objects in the freelist as TCMalloc~\cite{tcmalloc}, as discussed in Section~\ref{sec:movement}. 

%. Memory locality is defined as whether an object is allocated from the local physical node of the requesting thread. Most existing NUMA-aware allocators only ensure the locality of every object's first memory allocation~\cite{tcmallocnew, kim2013node, yang2019jarena}.  Instead, \NM{} additionally ensures the locality of all freed objects, eliminating the confusion caused by memory reuse (a very common behavior). \NM{} guarantees that a freed object will not be placed into a freelist belonging to a remote node. In particular, \NM{}'s thread binding allows it to determine the thread's physical node, and \NM{}'s origin-computable design can quickly identify each object's origin by the address's range. Then a freed object will be placed into the deallocating thread's local buffer (heap) \textit{only if} it is originated from the same node that the current thread is running on; otherwise, it will be returned back to its original node. 


%\NM{} utilizes the node-interleaved thread binding (binding threads to NUMA nodes interleavedly) by default, but it also supports node-saturate thread binding that will assign the same number of threads as the number of cores to a node before assigning threads to the next node. 
%In the future, we plan to support user-defined thread binding via the environment variable or a configuration file.
%Based on our experiments on multiple allocators, the node-interleaved thread binding performs the best performance when all nodes are used to run one application.
%\textbf{Experimental methodology and artifact availability.} We have performed an extensive evaluation on synthetic and real applications, with 24 applications in total. Compared \NM{} with popular allocators, such as the default Linux allocator, TCMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, Intel TBB~\cite{tbb}, Scalloc~\cite{Scalloc}, and mimalloc~\cite{mimalloc},  \NM{} is running 17\% faster than the second-best allocator (mimalloc), achieving around 19\% speedup comparing to the default Linux allocator. For the best case, \NM{} runs up to $6.4\times$ faster than the default allocator. \NM{} is much more scalable than all other allocators based on our evaluation. \NM{} is ready for practical employment, due to its high performance and good scalability. The code will be opened source using GNU GPL V2 license at \url{https://github.com/XXX} (opened upon acceptance). 

%\textbf{Limitations of the proposed approach.} The proposed work may need some user customization via configuration flags, such as interleaved heap or a specific format of thread binding, based on memory patterns of user applications. Some of these mechanisms, e.g. interleaved heap, are not universally beneficial to the performance of all applications. However, they do improve the performance of some applications. Therefore, we decide to keep such options, instead of discarding them. 

Overall, this paper makes the following contributions:

\begin{itemize}

\item It proposes a \textbf{binding-based memory management} to eliminate unnecessary remote accesses caused by thread migrations.

%\item It proposes an \textbf{origin-aware memory management} to ensure full locality of memory allocations. 

\item It proposes a \textbf{threads-shared incremental allocation} to achieve a better balance between the performance and memory consumption of huge pages. 

%\item It further proposes some other mechanism

\item It presents the design and implementation of \NM{}, which has a better performance and scalability than even widely-used commercial allocators. Overall, applications with \NM{} are running about 17\% faster than those with TCMalloc, but without using more memory.
%, such as TCMalloc, jemalloc, and Intel TBB, based on our extensive evaluation.  

\end{itemize}

%The remainder of this paper is organized as follows. Section~\ref{sec:background} introduces the OS support for the NUMA architecture and common designs of memory allocators. Section~\ref{sec:implement} focuses on the design and implementation of \NM{}. After that, Section~\ref{sec:evaluation} describes its experimental evaluation, and Section~\ref{sec:limit} discusses the limitation of \NM{}. In the end, Section~\ref{sec:related} discusses some relevant work, and then Section~\ref{sec:conclusion} concludes. 