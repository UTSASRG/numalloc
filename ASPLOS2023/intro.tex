
\section{Introduction}
\label{sec:intro}


Non-Uniform Memory Access (NUMA) is the de-facto design for modern many-core machines in order to address the scalability issues of increasing hardware cores. In NUMA architecture, each processor (or node/socket interchangeably) has its own memory, allowing threads running on different nodes accessing their own memory concurrently. Unfortunately, it is challenging to achieve the expected scalability. One notorious performance issue is caused by remote accesses that a task accesses the memory of a remote node (called remote memory), since a remote access has much higher latency than accessing the memory from the local node (or a local access)~\cite{Blagodurov:2011:CNC:2002181.2002182}. Although many profiling tools are proposed to identify NUMA issues of applications~\cite{Intel:VTune, Memphis, Lachaize:2012:MMP:2342821.2342826, XuNuma, NumaMMA, 7847070, NumaPerf}, they only focus on issues inside a memory object, while still requires memory allocators to ensure the locality of memory allocations/deallocations. 

In order to reduce remote accesses, some NUMA-aware allocators~\cite{tcmallocnuma, kim2013node, yang2019jarena} have been proposed in the past. Kaminski built the first NUMA-aware memory allocator on top of TCMalloc in 2008~\cite{tcmallocnuma}, called \TN{} in the remainder of this paper, where \TN{}'s designs are integrated into the modern TCMalloc~\cite{tcmalloc2} based on our analysis. \TN{} adds a freelist and page-span for each NUMA node, and binds the physical memory of each page-span to a specific physical node. \TN{} improves the locality by allocating objects from the per-node list/page-span that a thread is running on. However, \TN{} or TCMalloc typically return a freed object to the deallocating thread's freelist (without acquiring the lock), a common design shared by most allocators to reduce the synchronization overhead~\cite{Hoard}. Such a design will introduce the locality issue, when the deallocating thread is located on a different node from the allocating thread.  

%These designs are also borrowed by general-purpose allocators for their NUMA support, such as TCMalloc~\cite{tcmalloc2} and mimalloc~\cite{mimalloc}. 
%\todo{Similarly, TCMalloc designs a page allocator for each NUMA node so that a thread is allocating the physical memory from the page node.}
mimalloc~\cite{mimalloc}'s design alleviates this problem, although its initial target is not to solve the locality issue of NUMA architecture. It proposes per-page (e.g., 64K) free lists that are typically allocated/used by a single thread at a time, and all objects from the same page (even deallocated by other threads) will be returned to the same per-page free list so that they can be re-used by the same thread later. 
%, which \todo{not clear, change to: On the other hand, mimalloc~\cite{mimalloc} manages freelists from page granularity and each page has an extra thread-free list that is utilized to handle deallocation from remote threads, which could alleviate the above-mentioned problem.}


However, \textit{none of existing allocators achieves the full locality of memory allocations}, as they did not handle unexpected thread migrations between different nodes. When a thread is migrated to a different node, the migration will turn all of its local accesses to remote ones, introducing remote accesses. Further, existing allocators do not take advantage of the location relationship between threads and memory when assigning metadata, handling memory allocations/deallocations, and sharing huge pages. In order to address these issues, we propose a novel allocator, \NM{}, with the following novel contributions.  


First, \NM{} proposes a \textbf{binding-based memory management} that also binds threads to physical nodes, in addition to memory binding, with the following benefits: (1) the bindings enable \NM{} to obtain the origin (or the physical node) of memory and threads with few instructions inside the user space, without the need of invoking expensive system calls (around  $10,000$ cycles for getting the node of the memory), enabling more advanced memory management as discussed below; %Therefore, it is infeasible to perform more advanced memory management, as the performance benefits of reducing remote accesses will be canceled out by the cost of getting the origin information via system calls.
(2) thread binding eliminates remote accesses caused by the above-mentioned thread migrations, but it does not exclude OS-based scheduling. \NM{} only binds a thread to a specific node, and also allows users to specific the binding explicitly; and (3) thread binding reduces the locality issues of heap metadata, such as the mapping of identifying the size of each object, which could be always allocated from the same physical node. Note that while past systems~\cite{li2013numa, XuNuma, Lepers:2015:TMP:2813767.2813788} have exploited thread binding to improve the performance of NUMA applications, \NM{} is the first allocator that implements and utilizes both memory and threading binding inside the memory allocator. 

Second, \NM{} ensures the full locality of memory allocations with its \textbf{origin-aware memory management}, based on memory and thread binding. In particular, \textit{\NM{} ensures that each thread will always allocate the memory from the same physical node that the thread is running on}. Since \NM{} binds the virtual memory to physical nodes explicitly and maintains the mapping relationship internally, it could always infer the physical node (or origin) of each object. During deallocations, \NM{} guarantees that a freed object will be always placed into a freelist with the same origin as the allocating thread. That is, an object is returned to the deallocating thread \textit{only if} the object is originated from the same node as the current thread; otherwise, it will be returned to its original node (e.g., per-node free list). Because of this reason, \NM{} can ensure that a thread will always allocate objects originating from the same node, as per-thread and per-node free lists only hold local objects. In addition, \NM{} ensures that all heap metadata exists on the same local node, such as the mapping of tracking the size of objects.




 %\NM{} ensures that all heap objects are allocated from the local physical node of the allocating thread by handling deallocations of freed objects additionally. 
%As mentioned before, existing allocators at best ensures the locality of every objectâ€™s first memory allocation~\cite{tcmallocnuma, tcmalloc, yang2019jarena}. Additionally, 
%\NM{} ensures the locality of all freed objects, eliminating the confusion caused by memory reuse (a very common behavior). Different from existing work,
 
%Note that \NM{}'s origin-aware mechanism is different from mimalloc~\cite{mimalloc} or snmalloc~\cite{Snmalloc}. 
%which determines one deallocation is remote from thread granularity and each deallocation should always return to the allocating thread's freelist, regardless of whether those threads are running on the same node. Such design guarantees the locality of deallocation, but at the cost of higher contention of each thread's freelist.

% simply return freed objects to their original free lists, which is easier to achieve. \NEW{mimalloc considers the locality of deallocation, but it's done from thread level, which makes the threads running on the same node complete for the lock.}

Last but not least, \NM{} proposes a new \textbf{incremental sharing} mechanism to take advantage of the Transparent Huge Pages (THP) of modern hardware~\cite{hugepage}. Huge pages are expected to significantly reduce Translation Lookaside Buffer (TLB) misses, as each page table entry covers a larger range of virtual addresses (e.g., 2MB instead of 4KB). However, most existing allocators~\cite{dlmalloc, Hoard, Scalloc} do not support huge pages, or even require to disable huge pages~\cite{scallochugepage}.  Allocators supporting huge pages have their own shortcomings: LLAMA~\cite{LLAMA} allocates objects from huge pages based on the liveness of objects, but requires expensive analysis and profiling; TEMERAIRE~\cite{TEMERAIRE} does not share huge pages between different threads, since mistakenly letting two remote threads share the same huge page may impose some performance degradation.  
In contrast, \NM{} allows different threads running on the same node share the same huge page, based on explicit binding of threads. 
%with different size classes running on the same physical node, 
%Internally, \NM{} always prefers to utilize huge pages: it not only allows multiple threads to share the same huge page, but also 
\NM{} also shares the same huge page between different size classes of different threads incrementally: each thread will acquire either 32KB or an object (if the object is larger than 32KB) each time from the current huge page, instead of one whole huge page. Because it allows different threads and different size classes to share the same huge page, it practically minimizes the internal fragmentation of using huge pages.  
%\NM{} supports the \textbf{incremental sharing} that each thread will get few objects at a time, instead of one whole huge page, in order to reduce memory fragmentation. 
%As it is very expensive to track whether a huge page with small objects is completely deallocated, the proposed work will use different huge pages for big and small objects separately, which allows us to only track the full status of huge pages for big objects. 
% To make the OS use huge pages inherently, \NM{} maps a large region of memory (larger than the size of a huge page) for each node initially. 
Therefore, \NM{} combines the best of both worlds that it takes the performance advantage of huge pages but does not deteriorate the memory consumption. 


%\NM{} also has other implementation novelty. 
%It designs an efficient mechanism that move objects between different freelists, without traversing all objects in the freelist as TCMalloc~\cite{tcmalloc}, as discussed in Section~\ref{sec:movement}. 

%. Memory locality is defined as whether an object is allocated from the local physical node of the requesting thread. Most existing NUMA-aware allocators only ensure the locality of every object's first memory allocation~\cite{tcmallocnew, kim2013node, yang2019jarena}.  Instead, \NM{} additionally ensures the locality of all freed objects, eliminating the confusion caused by memory reuse (a very common behavior). \NM{} guarantees that a freed object will not be placed into a freelist belonging to a remote node. In particular, \NM{}'s thread binding allows it to determine the thread's physical node, and \NM{}'s origin-computable design can quickly identify each object's origin by the address's range. Then a freed object will be placed into the deallocating thread's local buffer (heap) \textit{only if} it is originated from the same node that the current thread is running on; otherwise, it will be returned back to its original node. 


%\NM{} utilizes the node-interleaved thread binding (binding threads to NUMA nodes interleavedly) by default, but it also supports node-saturate thread binding that will assign the same number of threads as the number of cores to a node before assigning threads to the next node. 
%In the future, we plan to support user-defined thread binding via the environment variable or a configuration file.
%Based on our experiments on multiple allocators, the node-interleaved thread binding performs the best performance when all nodes are used to run one application.
%\textbf{Experimental methodology and artifact availability.} We have performed an extensive evaluation on synthetic and real applications, with 24 applications in total. Compared \NM{} with popular allocators, such as the default Linux allocator, TCMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, Intel TBB~\cite{tbb}, Scalloc~\cite{Scalloc}, and mimalloc~\cite{mimalloc},  \NM{} is running 17\% faster than the second-best allocator (mimalloc), achieving around 19\% speedup comparing to the default Linux allocator. For the best case, \NM{} runs up to $6.4\times$ faster than the default allocator. \NM{} is much more scalable than all other allocators based on our evaluation. \NM{} is ready for practical employment, due to its high performance and good scalability. The code will be opened source using GNU GPL V2 license at \url{https://github.com/XXX} (opened upon acceptance). 

%\textbf{Limitations of the proposed approach.} The proposed work may need some user customization via configuration flags, such as interleaved heap or a specific format of thread binding, based on memory patterns of user applications. Some of these mechanisms, e.g. interleaved heap, are not universally beneficial to the performance of all applications. However, they do improve the performance of some applications. Therefore, we decide to keep such options, instead of discarding them. 

Overall, this paper makes the following contributions:

\begin{itemize}

 \item It proposes the first \textbf{binding-based memory management} to support NUMA architecture.

\item It proposes an \textbf{origin-aware memory management} to ensure the full locality of memory allocations. 

\item It proposes an \textbf{incremental sharing} to achieve a better balance between the performance and memory consumption for huge pages. 

%\item \NM{} proposes a binding-based memory allocator that supports NUMA architecture.

%\item Multiple mechanisms are proposed on top of binding, including incremental sharing, origin-aware memory management and interleaved heap. 

\item Experimental results on both normal and HPC applications show that \NM{} achieves better performance and scalability than all widely-used commercial allocators. 

% \item It further proposes multiple mechanisms, including origin-aware memory management, interleaved heap, and efficient data structure for  moving objects between different freelists. 

% \item It presents the design and implementation of \NM{}, which has a better performance and scalability than all widely-used commercial allocators. 
% Overall, applications with \NM{} are running about 18\% faster than the second-best allocator, but with a reasonable memory overhead.

%, such as TCMalloc, jemalloc, and Intel TBB, based on our extensive evaluation.  

\end{itemize}

%The remainder of this paper is organized as follows. Section~\ref{sec:background} introduces the OS support for the NUMA architecture and common designs of memory allocators. Section~\ref{sec:implement} focuses on the design and implementation of \NM{}. After that, Section~\ref{sec:evaluation} describes its experimental evaluation, and Section~\ref{sec:limit} discusses the limitation of \NM{}. In the end, Section~\ref{sec:related} discusses some relevant work, and then Section~\ref{sec:conclusion} concludes. 