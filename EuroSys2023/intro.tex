
\section{Introduction}
\label{sec:intro}


Non-Uniform Memory Access (NUMA) is the de-facto design for modern many-core machines in order to address the scalability issues of increasing hardware cores. In NUMA architecture, each processor (or called node/socket interchangeably) has its own memory, allowing threads running on different nodes to access their own memory concurrently. Unfortunately, it is challenging to achieve the expected scalability. One notorious performance issue is caused by remote accesses that a task accesses the memory of a remote node (called remote memory), since a remote access has much higher latency than accessing the memory from the local node (or a local access)~\cite{Blagodurov:2011:CNC:2002181.2002182}. Although many profiling tools are proposed to identify NUMA issues of applications~\cite{Intel:VTune, Memphis, Lachaize:2012:MMP:2342821.2342826, XuNuma, NumaMMA, 7847070, NumaPerf}, they typically focus on issues inside a memory object, while still requiring memory allocators to ensure the locality of memory allocations/deallocations. 

In order to reduce remote accesses, some NUMA-aware allocators~\cite{tcmallocnuma, kim2013node, yang2019jarena} have been proposed in the past. Kaminski built the first NUMA-aware memory allocator on top of TCMalloc in 2008~\cite{tcmallocnuma}, called \TN{} in the remainder of this paper, which has been integrated into the modern TCMalloc~\cite{tcmalloc2} based on our analysis. \TN{} adds a freelist and page-span for each NUMA node, and binds the physical memory to a physical node explicitly. \TN{} improves the locality by allocating objects from the per-node list/page-span that a thread is running on. mimalloc~\cite{mimalloc} proposes per-page (e.g., 64K) freelists that are typically allocated/used by a single thread, which increases the locality.

% \todo{Different from \TN{}, mimalloc handles remote deallocations specifically by adding a new per-page freelist.}

However, existing designs are not sufficient to eliminate remote accesses: (1) although in theory an allocator can always check a thread's physical node (e.g., via the system call) so that a thread only allocates the memory from its local node, that is unfortunately too expensive due to the high overhead of the system call (around  $10,000$ cycles for getting the node of the memory); (2) Some common practice of existing allocators also weakens the locality guarantee: each thread typically tracks objects deallocated by itself, and prefers the available memory in its local cache upon memory allocations. However, there is no guarantee that an object deallocated by a thread is originally from the thread's local physical node; (3) Threads can be migrated by the underlying operating system from one node to another, where the migration will turn all of a thread's local accesses to remote ones. (4) Existing allocators do not take advantage of the location relationship between threads and memory when handling memory allocations/deallocations, initializing metadata, and sharing huge pages.

%However, \TN{} or TCMalloc typically return a freed object to the deallocating thread's freelist (without acquiring the lock), a common design shared by most allocators to reduce the synchronization overhead~\cite{Hoard}. Such a design will introduce the locality issue, when the deallocating thread is located on a different node from the allocating thread.  
%These designs are also borrowed by general-purpose allocators for their NUMA support, such as TCMalloc~\cite{tcmalloc2} and mimalloc~\cite{mimalloc}. 
%\todo{Similarly, TCMalloc designs a page allocator for each NUMA node so that a thread is allocating the physical memory from the page node.}
%mimalloc~\cite{mimalloc}'s design alleviates this problem, although its initial target is not to solve the locality issue of NUMA architecture. It 
%, which \todo{not clear, change to: On the other hand, mimalloc~\cite{mimalloc} manages freelists from page granularity and each page has an extra thread-free list that is utilized to handle deallocation from remote threads, which could alleviate the above-mentioned problem.}



%However, \textit{none of existing allocators achieves the full locality of memory allocations}, as they did not handle unexpected thread migrations between different nodes. When a thread is migrated to a different node, the migration will turn all of its local accesses to remote ones, introducing higher latency caused by remote accesses. Further, existing allocators do not take advantage of the location relationship between threads and memory when handling memory allocations/deallocations, assigning metadata, and sharing huge pages, \todo{which makes them miss the opportunities to improve the performance.} In order to address these issues, we propose a novel allocator, \NM{}, with the following novel contributions.  


To address these issues, we propose a novel \textbf{binding-based allocator}, called \NM{}.
%ensures the full-locality of memory allocations. Different from existing allocators, \NM{} 
\NM{} performs thread-binding and memory-binding inside the allocator at the same time. The thread-binding provides the following benefits: (1) it enables \NM{} to obtain the origin (or the physical node) of threads with few instructions without using expensive system calls, as threads are always staying at the same node; (2) Thread binding eliminates remote accesses caused by the above-mentioned thread migrations. Note that \textit{\NM{}'s thread-binding binds a thread to a specific node, instead of a particular core}. It does not exclude OS-based scheduling, where the OS could still schedule based on the changed resource. By default, \NM{} takes the node-interleaved binding that binds continuous threads to different nodes in an interleaved way so that every node will have a similar number of threads. Given such a node balanced binding, we argue that \NM{} can be also employed in the server environment with thread-pool design.
Further, \NM{} could also support node-saturate binding or any explicit binding provided by users, allowing users to provide more control. The combination of thread-binding and memory-binding inside the allocator provides a clear relationship between threads and memory. Therefore, it enables more advanced memory management discussed as follows, addressing the other two above-mentioned issues. 
% as discussed below;
%, but we argue that 

%enabling more advanced memory management \todo{such as origin-aware memory management and incremental sharing mechanism;}
% as discussed below; %Therefore, it is infeasible to perform more advanced memory management, as the performance benefits of reducing remote accesses will be canceled out by the cost of getting the origin information via system calls.

%  \NM{} only binds a thread to a specific node, and also allows users to specific the binding explicitly; 
%and (3) thread binding reduces the locality issues of heap metadata, such as the mapping for identifying the size of each object, which could be always allocated from the same physical node. 

Based on memory and thread binding, \NM{} ensures the full locality of memory allocations with its \textbf{origin-aware memory management}. \NM{} guarantees that each thread will always allocate the memory from the same physical node that the thread is running on. Since \NM{} binds the virtual memory to physical nodes explicitly and maintains the mapping relationship internally, it could always infer the physical node (or origin) of each object. During deallocations, \NM{} guarantees that a freed object will be always placed into a freelist with the same origin as the allocating thread. In particular, an object is returned to the deallocating thread \textit{only if} the object is originated from the same node as the current thread; otherwise, it will be returned to its original node (e.g., per-node free list). 
%In this way, \NM{} can ensure that a thread will always allocate objects originating from the same node, as per-thread and per-node free lists only hold local objects. 
In addition, \NM{} ensures that all heap metadata exists on the same local node, such as the metadata of tracking the size of objects.




 %\NM{} ensures that all heap objects are allocated from the local physical node of the allocating thread by handling deallocations of freed objects additionally. 
%As mentioned before, existing allocators at best ensures the locality of every objectâ€™s first memory allocation~\cite{tcmallocnuma, tcmalloc, yang2019jarena}. Additionally, 
%\NM{} ensures the locality of all freed objects, eliminating the confusion caused by memory reuse (a very common behavior). Different from existing work,
 
%Note that \NM{}'s origin-aware mechanism is different from mimalloc~\cite{mimalloc} or snmalloc~\cite{Snmalloc}. 
%which determines one deallocation is remote from thread granularity and each deallocation should always return to the allocating thread's freelist, regardless of whether those threads are running on the same node. Such design guarantees the locality of deallocation, but at the cost of higher contention of each thread's freelist.

% simply return freed objects to their original free lists, which is easier to achieve. \NEW{mimalloc considers the locality of deallocation, but it's done from thread level, which makes the threads running on the same node complete for the lock.}

Based on thread-binding, \NM{} proposes a new \textbf{incremental sharing} mechanism to take advantage of the Transparent Huge Pages (THP) of modern hardware~\cite{hugepage}. Huge pages are expected to significantly reduce Translation Lookaside Buffer (TLB) misses, as each page table entry covers a larger range of virtual addresses (e.g., 2MB instead of 4KB). However, most existing allocators~\cite{dlmalloc, Hoard, Scalloc} do not support huge pages, or even require to disable huge pages~\cite{scallochugepage}.  Allocators supporting huge pages have their own shortcomings: LLAMA~\cite{LLAMA} allocates objects from huge pages based on the liveness of objects, but requires expensive analysis and profiling; TEMERAIRE~\cite{TEMERAIRE} does not share huge pages between different threads, since mistakenly letting two remote threads share the same huge page may impose some performance degradation.  
In contrast, \NM{}'s thread binding makes it possible to share the huge pages between threads running on the same node. We call it ``incremental sharing'' as each thread only fetches the amount of memory it needs from the current page at a time, instead of the entire page.
%This sharing also applies to objects of different size classes, which further reduces the memory fragmentation.
% In contrast, \NM{} allows different threads running on the same node share the same huge page, based on explicit binding of threads. 
%with different size classes running on the same physical node,
% \NM{} also shares the same huge page between different size classes of different threads incrementally: each thread will acquire either 32KB or an object (if the object is larger than 32KB) each time from the current huge page, instead of one whole huge page. Because it allows different threads and different size classes to share the same huge page, it practically minimizes the internal fragmentation of using huge pages.  
%\NM{} supports the \textbf{incremental sharing} that each thread will get few objects at a time, instead of one whole huge page, in order to reduce memory fragmentation. 
%As it is very expensive to track whether a huge page with small objects is completely deallocated, the proposed work will use different huge pages for big and small objects separately, which allows us to only track the full status of huge pages for big objects. 
% To make the OS use huge pages inherently, \NM{} maps a large region of memory (larger than the size of a huge page) for each node initially. 
Therefore, \NM{} combines the best of both worlds that it takes the performance advantage of huge pages but does not deteriorate the memory consumption. 


%\NM{} also has other implementation novelty. 
%It designs an efficient mechanism that move objects between different freelists, without traversing all objects in the freelist as TCMalloc~\cite{tcmalloc}, as discussed in Section~\ref{sec:movement}. 

%. Memory locality is defined as whether an object is allocated from the local physical node of the requesting thread. Most existing NUMA-aware allocators only ensure the locality of every object's first memory allocation~\cite{tcmallocnew, kim2013node, yang2019jarena}.  Instead, \NM{} additionally ensures the locality of all freed objects, eliminating the confusion caused by memory reuse (a very common behavior). \NM{} guarantees that a freed object will not be placed into a freelist belonging to a remote node. In particular, \NM{}'s thread binding allows it to determine the thread's physical node, and \NM{}'s origin-computable design can quickly identify each object's origin by the address's range. Then a freed object will be placed into the deallocating thread's local buffer (heap) \textit{only if} it is originated from the same node that the current thread is running on; otherwise, it will be returned back to its original node. 


%\NM{} utilizes the node-interleaved thread binding (binding threads to NUMA nodes interleavedly) by default, but it also supports node-saturate thread binding that will assign the same number of threads as the number of cores to a node before assigning threads to the next node. 
%In the future, we plan to support user-defined thread binding via the environment variable or a configuration file.
%Based on our experiments on multiple allocators, the node-interleaved thread binding performs the best performance when all nodes are used to run one application.
%\textbf{Experimental methodology and artifact availability.} 
We have performed an extensive evaluation on synthetic and real applications, with 25 applications in total. Compared \NM{} with popular allocators, such as the default Linux allocator, TCMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, Intel TBB~\cite{tbb}, Scalloc~\cite{Scalloc}, and mimalloc~\cite{mimalloc},  \NM{} is running around 16\% faster than the second-best allocator (mimalloc), achieving around 21\% speedup comparing to the default Linux allocator. For the best case, \NM{} runs up to $5.3\times$ and $4.6\times$ faster than the default allocator and mimalloc. At the same time, \NM{}'s memory consumption is comparable to industrial-level allocators, such as TCMalloc, jemalloc, and mimalloc. \NM{} is much more scalable than all other allocators based on our evaluation. \NM{} is ready for practical employment, due to its high performance and good scalability. 
% The code will be opened source using GNU GPLV2 license at \url{https://github.com/XXX} (opened upon acceptance). 
Overall, this paper makes the following contributions:


%\textbf{Limitations of the proposed approach.} The proposed work may need some user customization via configuration flags, such as interleaved heap or a specific format of thread binding, based on memory patterns of user applications. Some of these mechanisms, e.g. interleaved heap, are not universally beneficial to the performance of all applications. However, they do improve the performance of some applications. Therefore, we decide to keep such options, instead of discarding them. 


\begin{itemize}

\item It proposes the first \textbf{binding-based memory management} to support the NUMA architecture.

\item It proposes an \textbf{origin-aware memory management} to ensure the full locality of memory allocations. 

\item It proposes an \textbf{incremental sharing} to achieve a better balance between the performance and memory consumption for huge pages. 

%\item \NM{} proposes a binding-based memory allocator that supports NUMA architecture.

%\item Multiple mechanisms are proposed on top of binding, including incremental sharing, origin-aware memory management and interleaved heap. 

\item Experimental results on both normal and HPC applications show that \NM{} achieves better performance and scalability than all widely-used commercial allocators. 

% \item It further proposes multiple mechanisms, including origin-aware memory management, interleaved heap, and efficient data structure for  moving objects between different freelists. 

% \item It presents the design and implementation of \NM{}, which has a better performance and scalability than all widely-used commercial allocators. 
% Overall, applications with \NM{} are running about 18\% faster than the second-best allocator, but with a reasonable memory overhead.

%, such as TCMalloc, jemalloc, and Intel TBB, based on our extensive evaluation.  

\end{itemize}

%The remainder of this paper is organized as follows. Section~\ref{sec:background} introduces the OS support for the NUMA architecture and common designs of memory allocators. Section~\ref{sec:implement} focuses on the design and implementation of \NM{}. After that, Section~\ref{sec:evaluation} describes its experimental evaluation, and Section~\ref{sec:limit} discusses the limitation of \NM{}. In the end, Section~\ref{sec:related} discusses some relevant work, and then Section~\ref{sec:conclusion} concludes. 