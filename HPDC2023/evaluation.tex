\section{Experimental Evaluation}
\label{sec:evaluation}

This section aims to answer the following research questions: 

\begin{itemize}
\item \textbf{Performance:} How is \NM{}'s performance, comparing to existing general allocators and NUMA-aware allocators? (Section~\ref{sec:performance}) 
\item \textbf{Memory Consumption:} What is the memory consumption of \NM{}? (Section~\ref{sec:memory})
\item \textbf{Scalability:} How is the scalability of \NM{}? (Section~\ref{sec:scale})
\item \textbf{Impact of Design Choices:} What is the impact of each design choice on the performance of \NM{}? (Section~\ref{sec:design})	
\end{itemize}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=7in]{figure/8-node-perf.jpg}
    \caption{Performance of different allocators on PARSEC, OpenMP/MPI and real applications, where all data\\ are normalized to that of the default Linux allocator. Here, a lower bar indicates a better performance. \\ Note that some applications are failed to run with some allocators.
    % \todo{To adding the error-bars.}
    \label{fig:perf1}}
 \end{figure*}
 
 
 \begin{figure*}[!ht]
    \centering 
    \includegraphics[width=7in]{figure/remote-access.jpg}
    \caption{Normalized runtime, remote accesses, and TLB misses of different allocators, where \\ the remote accesses and TLB misses data are normalized to that of \NM{} and runtime data is normalized to \\ that of the default Linux allocator for better comparison of line trend. All data are finally log-transformed to limit the range of data distribution. Here, a lower runtime indicates a better performance.}
    \label{fig:remoteAccess}
\end{figure*}



\begin{comment}

\begin{table}[!ht]
 \centering
   \caption{Machine specifications for evaluation
   \label{table:Machine}}
  %\setlength{\tabcolsep}{1.0em}
\begin{tabular}{l | l }
\hline
Category & Information \\ \hline
CPUs/Model 	& Xeon(R) Platinum 8153\\ \hline
CPU Frequency & 2.00GHz\\ \hline
NUMA Nodes  & 8 \\ \hline
Physical Cores  & 8$\times$16 \\ \hline
Node Latency &  \specialcell{local: 1.0 \\ 1 hop: 2.1 \\ 2 hops: 3.1}\\ \hline
Interconnect Bandwidth  & 10.4GT/s\\ \hline
Linux & Debian 10\\ \hline
Compiler &  GCC-8.3.0 \\ \hline
%Memory Bandwidth & 19.87 GB/s & \\ \hline
  \end{tabular}
  %\vspace{-0.4in}
\end{table}
\end{comment}

\textbf{Experimental Setup:}  \NM{} was evaluated on an Intel Xeon(R) Platinum 8153 machine with 8 nodes, where each node has 16 cores. 8 nodes are divided into two groups, where the four nodes of each group are fully connected, and there are four links between the two groups. Any two nodes are less than or equal to 2 hops, where the latency of one hop and two hops is 2.1 and 3.1 separately if the latency of local accesses is 1.0. The machine is installed with 512GB memory. Each core has a dedicated 32KB L1 and 1MB L2 cache, and cores within a node share a 22MB last-level cache (LLC). The underlying OS is Linux Debian 10 and the compiler is GCC-8.3.0. In the evaluation, the hyperthreading was turned off, but both transparent huge page and AutoNUMA are enabled. The performance data shown in this paper is an average of 30 executions, in order to avoid any bias caused by unexpected events.

\textbf{Compared Allocators: } We compare \NM{} with multiple popular allocators, such as the default Linux allocator (Glibc-2.28)~\cite{glibcweb}, TCMalloc~\cite{tcmalloc2},  TCMalloc-NUMA~\cite{tcmallocnuma}, jemalloc-5.2.1~\cite{jemalloc}, Intel TBB-2021.5~\cite{tbb2}, Scalloc-1.0.0~\cite{Scalloc}, and mimalloc-1.6.7~\cite{mimalloc}. Note that we are comparing against TCMalloc's NUMA awareness version (released in July 2021), and the newest version of TBB with NUMA support (released in December 2021). The evaluated TCMalloc already includes TEMERAIRE~\cite{TEMERAIRE}'s huge page support. 
%Among them, TCMalloc, jemalloc, TBB, and mimalloc are commercial allocators designed and maintained by Industrial Giants, like Google, Facebook, Intel, and Microsoft separately. 
We do not include Hoard~\cite{Hoard} as it is not the state-of-art anymore~\cite{Scalloc, mimalloc}. 

\textbf{Evaluated Applications: }  We evaluated the PARSEC applications~\cite{parsec}, five OpenMP/MPI applications \texttt{Nekbone}, \texttt{QMCPACK}, \texttt{LAMMPS}, \texttt{AMG} and \texttt{Quicksilver} from CORAL-2 Benchmarks~\cite{coral2}, and real applications including \texttt{Apache httpd-2.4.35}, \texttt{MySQL-5.7.15}, \texttt{Memcached-1.4.25}, \texttt{SQLite-3.12.0}, \texttt{Aget}, \texttt{Pfscan} and \texttt{Pbzip2}. PARSEC applications are using native inputs~\cite{parsec}. For \texttt{Apache}, we use the \texttt{ab} script to send 1,000,000 requests~\cite{apachetest}. For \texttt{MySQL}, we use \texttt{sysbench} with 128 threads separately, each issuing 100,000 requests. For \texttt{Memcached}, the \texttt{python-memcached} is used to evaluate it with 3000 loops to get the sufficient runtime~\cite{memcached}. \texttt{Aget} is tested by downloading a 30-MB file, and \texttt{Pfscan} is tested by searching  a keyword in a 500MB data. In terms of \texttt{Pbzip2}, we test it by compressing 10 files with 30MB each. Finally, \texttt{SQLite} is tested through a program called \texttt{threadtest3}~\cite{sqlitetest}. We utilize 128 threads, which is the same as the number of cores of the evaluated machine. For OpenMP/MPI applications, we use a hybrid MPI + OpenMP mode where the number of MPI processes is equal to the number of nodes and each process has 16 OpenMP threads, which is the core number inside one node.
% For input dataset, \texttt{QMCPACK} uses \texttt{heg-short-HF.xml} as input and others are all using the same input provided as the benchmark.

% Multithreaded applications chosen to evaluate the performance include PARSEC applications~\cite{parsec}, five OpenMP/MPI applications from CORAL-2 Benchmarks~\cite{coral2}, including \texttt{Nekbone}, \texttt{QMCPACK}, \texttt{LAMMPS}, \texttt{AMG} and \texttt{Quicksilver}, and real applications such as \texttt{Apache httpd-2.4.35}, \texttt{MySQL-5.7.15}, \texttt{Memcached-1.4.25}, \texttt{SQLite-3.12.0}, \texttt{Aget}, \texttt{Pfscan} and \texttt{Pbzip2}.


%  \begin{figure}[!h]
%     \centering
%     \includegraphics[width=3.5in]{figure/8-node-mpi-perf.jpg}
%     \caption{Performance of different allocators on OpenMP/MPI applications, where all data is normalized to that of the default Linux allocator. Note that some applications are failed to run with some allocators.
%     \label{fig:perf2}}
%  \end{figure}



\begin{comment}
\begin{table}[!ht]
 \centering
  %\setlength{\tabcolsep}{1.0em}
\begin{tabular}{c | c | c}
\hline
System & \textbf{Machine A} & \textbf{Machine B} \\ \hline
CPUs/Model & Xeon Gold 6138	& Xeon(R) Platinum 8153\\ \hline
CPU Frequency & 2.10GHz & 2.00GHz\\ \hline
NUMA Nodes & 2 & 8 \\ \hline
Physical Cores & 2$\times$20 & 8$\times$16 \\ \hline
Node Latency & \specialcell{local: 1.0 \\ 1 hop: 2.1} & \specialcell{local: 1.0 \\ 1 hop: 2.1 \\ 2 hops: 3.1}\\ \hline
Interconnect Bandwidth & 8GT/s & 10.4GT/s\\ \hline
Linux & Ubuntu 18.04 & Debian 10\\ \hline
Compiler & GCC-7.5.0 & GCC-8.3.0 \\ \hline
%Memory Bandwidth & 19.87 GB/s & \\ \hline
  \end{tabular}
   \caption{Machine specifications for evaluation
   \label{table:Machine}}
  %\vspace{-0.4in}
\end{table}
\end{comment}

\subsection{Performance Evaluation}
\label{sec:performance}

% PARSEC applications are using native inputs~\cite{parsec}. For \texttt{MySQL}, we use \texttt{sysbench} with 128 threads separately, each issuing 100,000 requests. The \texttt{python-memcached}  is used to exercise \texttt{Memcached}, with 3000 loops to get the sufficient runtime~\cite{memcached}. The \texttt{ab} is used to test \texttt{Apache} server~\cite{apachetest}, by sending 1,000,000 requests in total. \texttt{Aget} is tested by downloading a 30-MB file, and \texttt{Pfscan} is tested by searching  a keyword in a 500MB data. In terms of \texttt{Pbzip2}, we test it by compressing 10 files with 30MB each. Finally, \texttt{SQLite} is tested through a program called \texttt{threadtest3}~\cite{sqlitetest}. 

%\todo{The number of threads of all benchmarks were adjusted according how many cores and nodes in the target machine to make threads could be properly distributed over the nodes and cores, making the number of threads as close as the number of cores. In the test machine, the thread number is 128.}

%In the Hoard~\cite{Hoard} benchmarks, we used 100 iterations and 1,280,000 64-byte objects for threadtest and also we run larson for 10 seconds with 1,000 7-2048 bytes object to cover all size classes in almost all allocators for 10,000 iterations.For false sharing , we used 100,000 inner-loop , 100,000 iterations with 8 bytes objects. 

%The number of threads of all benchmarks were adjusted according how many cores and nodes in the target machine to make threads could be properly distributed over the nodes and cores, making the number of threads as close as the number of cores. Mostly, thread number was 40 in the Machine A and 128 in the Machine B, and I will give the specific number below if it is not this default value. 

Figure~\ref{fig:perf1} shows the performance of normal applications and HPC applications with different allocators, respectively. The runtime of each allocator is normalized to that of Linux's default one. 
% \NM{} is configured without the interleaved heap support for most applications, except for \texttt{fluidanimate} and \texttt{streamcluster}. As evaluated in Section~\ref{sec:interleavedheap}, the interleaved heap will significantly improve the performance for these two applications. 
% \todo{Let's remove the interleaved heap support from everywhere}
%As further discussed in Section~\ref{sec:interleavedheap}, the interleaved heap may have a harmful performance impact for the serial phases, as it turns some local accesses to remote ones. %Therefore, the data of \texttt{canneal} and \texttt{raytrace} are collected without the interleaved heap. 
%We believe that this option is acceptable, since users could determine easily whether they should disable the interleaved heap: if an application has a large portion in the serial phase, then the interleaved heap should be disabled. 
%The impact of the interleaved heap is further discussed and evaluated in Section~\ref{sec:interleavedheap}. 
 %With the interleaved heap,  allocations from the main thread can be satisfied in remote NUMA nodes, this design may lead to a large number of remote accesses for the serial phase. since both of them spend a large portion of their time (over 62\% and 82\%) in the serial phase (before creating any child thread),
 %These two figures show the best data for these two applications, without the support of the interleaved heap.    
Overall, \NM{} has the best performance among these allocators. In particular, \NM{} is 15.7\% faster than the second-best allocator (mimalloc) and 20.9\% faster than the default Linux allocator. 
For the best case (e.g., \texttt{fluidanimate}), \NM{} is running up to $5.3\times$ faster than the default Linux allocator, and $4.6\times$ faster than mimalloc.
On average, \NM{} is 18.4\%, 18.2\%, and 20.7\% faster than TCMalloc~\cite{tcmalloc2}, TCMalloc-NUMA~\cite{tcmallocnuma}, Intel TBB~\cite{tbb3}, all of which are NUMA-aware allocators. Considering only OpenMP/MPI applications, they run 26.1\% faster with \NM{} compared with the default Linux allocator, and 16.5\% faster than the second-best allocator (mimalloc).

% \begin{figure}[!h]
%     \centering 
%     \includegraphics[width=3.5in]{figure/remote-access.jpg}
%     \caption{Normalized runtime, remote accesses, and TLB misses of different allocators for normal applications (to \NM{}), where the lower is the better. }
%     \label{fig:remoteAccess}
% \end{figure}

% \begin{figure}[!h]
%     \centering 
%     \includegraphics[width=3.5in]{figure/remote-access-hpc.pdf}
%     \caption{Normalized runtime, remote accesses, and TLB misses of different allocators for HPC applications (to \NM{}), where the lower is the better. }
%     \label{fig:remoteAccess2}
% \end{figure}

%\NM{} is running \todo{19\%} faster than the default allocator, but does not run significantly slower than other allocators in almost all evaluated applications. For the best case (e.g., \texttt{fluidanimate}), \NM{} is running up to \todo{$6.4\times$} faster than the default Linux allocator.
%The default Linux allocator achieves good performance on the NUMA architecture due to its arena-based design, where each freed object will be returned back to its original arena. This design is integrating well with Linux's first-touch allocation policy~\cite{Lameter:2013:NO:2508834.2513149} to ensure most thread-local allocations. 
% In contrast, other allocators typically utilize a per-thread cache to store objects that are deallocated by the current thread, which may lead to remote accesses as described in Section~\ref{sec:intro}.
% Comparing with the NUMA-aware allocator -- TCMalloc-NUMA~\cite{tcmallocnew}, \NM{} runs \todo{23\%} faster. 
%TCMalloc-NUMA, TCMalloc, TBB, and mimalloc are allocators that are claimed to support the NUMA architecture~\cite{tcmallocnew}. 
%But TCMalloc-NUMA's performance is even worse than that of TCMalloc. Based on our understanding, TCMalloc-NUMA is based on TCMalloc-0.97 (released in 2008), which does not have new features of TCMalloc-2.7 (the version for our evaluation). 
% mimalloc only has the very basic NUMA support, which is the reason why it is not performing as well as \NM{}. TCMalloc and TBB are allocators that are claimed to support the NUMA architecture~\cite{tcmalloc2, tbb3}, but \NM{} is \todo{21\%} faster than both of them.

As shown in Figure~\ref{fig:perf1}, \NM{} has a significant performance improvement (over 25\%) in the following PARSEC and real applications, including \texttt{dedup}, \texttt{fluidanimate}, \texttt{pbzip2}, \texttt{streamcluster}, and \texttt{swaptions}. For these applications, we further examine the number of remote accesses and TLB misses to confirm whether \NM{} significantly reduces them. 
% By ensuring the full locality of memory accesses, \NM{} is expected to reduce the number of remote accesses. \NM{} will reduce the TLB misses, since it takes the advantage of transparent huge pages.  
We utilize the \texttt{perf-4.19}~\cite{perfweb}, a Linux performance profiling tool, to collect these numbers. 
% Figure~\ref{fig:perf} shows that \NM{} has a similar or better performance than the default allocator in almost all applications. Further, it has a significant performance improvement (over 20\%) in the following applications, including \texttt{canneal}, \texttt{dedup}, \texttt{fluidanimate}, \texttt{streamcluster}, and \texttt{pbzip2}. Among them, \NM{} has a better performance for \texttt{fluidanimate}, due to the following reasons. First, the interleaved heap contributes to $3.23\times$ performance speedup, as shown in Figure~\ref{fig:interleavedheap}. Second, node-balanced thread binding improves the performance by over $4\times$. 
% \textbf{Confirming the number of remote accesses:} We further examine the number of remote accesses to confirm whether \NM{} significantly reduces them due to its design. We utilize the \texttt{perf} to collect the number of remote accesses (both load and store) on these applications when they are running with all allocators. 
%In our machine, we can collect these numbers using the following command: \texttt{perf stat -e node-load-misses,node-store-misses,dTLB-store-misses,dTLB-load-misses} 
The results are shown in Figure~\ref{fig:remoteAccess}, which includes the runtime performance (black line), remote accesses (blue line), and TLB misses plus remote accesses (red line) together for a better comparison. We do not plot the separate TLB miss data because it fluctuates too much across different allocators.

Overall, \NM{} significantly reduces the number of remote accesses for the evaluated applications. In particular, \NM{} has $9\times$ fewer remote accesses than the default Linux allocator and $8\times$ fewer than the second-best allocator (mimalloc) on average. For TLB misses, \NM{} reduces it by $18\times$ compared to the default Linux allocator. We also notice that, as expected, TCMalloc performs best among allocators other than \NM{} with a low number of TLB misses (about $1.7\times$ of \NM{}) due to its support for huge pages.
% We also notice that, as expected, since TCMalloc supports huge pages, it has fewer TLB misses and performs the best in allocators other than \NM{} (about $1.7\times$ of \NM{}).
As can be seen from Figure~\ref{fig:remoteAccess}, \NM{} greatly reduces the number of remote accesses for three applications, \texttt{fluidanimate}, \texttt{pbzip2}, and \texttt{streamcluster}. Let us utilize \texttt{fluidanimate} as an example, where \NM{} is running $4.8\times$ faster than TBB and $5.3\times$ faster than the default Linux allocator. Figure~\ref{fig:remoteAccess} shows that TBB and the default Linux allocator have $5.9\times$ and $6.2\times$ more remote accesses than \NM{}, which explains why \NM{} is the fastest on this application. \NM{}'s big reduction of remote accesses can be attributed to the following factors: its thread binding avoids unnecessary remote accesses; its metadata is placed on the local node, based on the binding design; its origin-aware memory allocation ensures locality of memory allocations.

However, for \texttt{dedup} and \texttt{swaptions}, there is not much difference in the number of remote accesses compared to some allocators, except for mimalloc on \texttt{swaptions}.
% However, there is not much difference in the number of remote accesses for \texttt{dedup} and \texttt{swaptions} compared to some allocators, except for mimalloc on \texttt{swaptions}, where \NM{} incurs $1.52\times$ more remote accesses. We do not know the exact reason for the low number of remote accesses on mimalloc. 
Based on our investigation, \NM{} is running faster than others on these two applications due to the reduction of TLB misses instead. 
Taking \texttt{dedup} as an example, compared to \TN{}, \NM{} generates 1.8\% more remote accesses, but it has more than $21\times$ fewer TLB misses, resulting in better performance. This is also true for \texttt{swaptions} compared with Scalloc and mimalloc. From Figure~\ref{fig:remoteAccess}, we notice that mimalloc has a surprisingly low number of remote accesses on \texttt{swaptions} compared to all other allocators. We currently do not know the exact reason for 
this. Nevertheless, \NM{} ended up performing slightly better due to $1.57\times$ fewer TLB misses.
% Compared with mimalloc on \texttt{swaptions}, \NM{} incurs 52\% more remote accesses. We do not know the exact reason for the low number of remote accesses on mimalloc. 
% For the cases of \texttt{swaptions}, although \NM{} has more remote accesses compared to mimalloc, it has $1.57\times$ fewer TLB misses, thus it performs slightly better. 
% Taking \texttt{swaptions} as an example, compared to mimalloc, \NM{} has $1.52\times$ more remote accesses, but it has $1.57\times$ fewer TLB misses, resulting in slightly better performance.
% For TLB misses, \NM{} is typically orders of magnitude lower than other allocators (except TCMalloc) for \texttt{pbzip2}. Especially, the default allocator and jemalloc have $49\times$ or $57\times$ more TLB misses.
If we look at remote accesses plus TLB misses data (red line) in Figure~\ref{fig:remoteAccess}, \NM{} always performs the best among all evaluated allocators.
Meanwhile, we observe that the remote accesses plus TLB misses (red line) generally has a positive correlation with application performance (black line). 
% Some applications show a similar magnitude of performance variation with remote accesses plus TLB misses (e.g. streamcluster), while others see little fluctuation in performance. For example, although \NM{} performs best in \texttt{Pbzip2}, the performance difference between allocators is quite small. Based on our understanding, \texttt{Pbzip2} is an IO-bound application so that the computation difference does not have a significant impact on its overall performance.
% Overall, \NM{} has either a lower number of remote accesses or TLB misses than other allocators, which explains why \NM{} is the fastest on these applications.

% Interestingly, the performance difference of \texttt{pbzip2} is even smaller than \texttt{fluidanimate}, given the large difference in TLB misses. Based on our understanding, \texttt{pbzip2} is an IO-bound application so that the computation difference does not have a large impact on its overall performance.

% We also notice that both \NM{} and TCMalloc have a lower number of TLB misses, as they have the support for huge pages.   

\input{memorytable-eurosys23}

We also conduct such experiments on the OpenMP/MPI applications as shown in Figure~\ref{fig:remoteAccess}. Here we only show applications that have significant performance improvement. Among three applications, \NM{} greatly reduces the remote accesses of \texttt{LAMMPS} and \texttt{AMG}, and therefore outperforms the other allocators. For \texttt{Quicksilver}, the number of remote accesses and TLB misses for \NM{} is comparable to some allocators, resulting in a relatively good performance. 
However, although \NM{} reduces the remote accesses and TLB misses by $1.62\times$ and $1.13\times$ compared to the second-best allocator (mimalloc), the performance is slightly worse. This is because other factors (e.g. cache misses) dominate the performance. But this is the only exception across all evaluations. In most cases, fewer remote accesses and TLB misses indicate better performance.
Overall, \NM{} has fewer remote accesses and fewer TLB misses, which are the reasons that \NM{} has better performance than other allocators on these applications. 
% Compared with the second-best allocator (mimalloc), the remote accesses of \NM{} are reduced by $1.62\times$ and the TLB misses are reduced by $1.13\times$ on Quicksilver. 
% However, mimalloc performs slightly better than \NM{} because other factors (e.g. cache miss rate) dominate performance.
% The data for other allocators are similar or even worse. This further confirms that \NM{} can reduce the number of remote accesses as well as TLB misses, thus improving the performance.
% Overall, \NM{} either has fewer remote accesses or fewer TLB misses, which are the reasons that \NM{} has better performance than other allocators on these applications. 


% To further validate the effectiveness of \NM{}, we also evaluate it on the Rodinia OpenMP benchmark suite~\cite{che2009rodinia}. According to the results, applications using \NM{} gain an average speedup of $2.33\times$ compared to the second-best allocator (mimalloc), and $2.44\times$ speedup compared to the default Linux allocator, which indicates that \NM{} also performs well on OpenMP applications. Due to the page limit, we do not show the runtime of each application in this paper. 

%However, there is no much difference in the number of remote accesses for \texttt{dedup} and \texttt{swaptions} compared with some allocators.  When transparent huge page support is enabled, \NM{} utilizes huge pages instead of small pages, leading to fewer TLB misses. For example, for \texttt{dedup}, \NM{}'s TLB misses is $2.82\times$ lower than that of TCMalloc-NUMA, under the similar number of remote accesses.

% \todo{check if there is a true causality between the amount of reduced remote access and the amount of performance improvement; Better insight into some of the experiments to understand why in about 40\% of the cases NUMAlloc doesn't clearly beat other allocators.}




%For TLB misses, typically both both \NM{} and TCMalloc have lower TLB misses than others, as they all take advantage of huge pages. For \texttt{pbzip2}, other allocators (e.g., TBB, glibc, Scalloc, and jemalloc) could be dozens of larger than \NM{}'s. \todo{We can also observe this trend in \texttt{dedup} application.}

% \NM{} also has much less TLB misses on \texttt{dedup} application.

%data in https://docs.google.com/spreadsheets/d/1WqWH5J7CcQuV8Vs4HnqRSC7TZpgQGPB9Gh2knHPSZPU/edit?usp=sharing
%We also observe some similar results for other applications. For \texttt{dedup}, the \texttt{glibc} allocator introduces 17850 time more TLB misses than \NM{}. Note that we only collect these two factors here, where the performance of allocators can be caused by other reasons, such as memory management operations.

%Further, \NM{} allocates a large chunk of memory initially, then the OS tends to utilize huge pages if possible, although this mechanism may introduce more memory consumption as evaluated in Section~\ref{sec:memory}.  


%\todo{Change "Performance" to runtime, also change the the performance/remote accesses" scale to 8, maybe put NUMAlloc to the first, change TcMalloc to TCMalloc. }

\begin{comment}
\begin{table}[htp]
    \centering
    \footnotesize
    \begin{tabular}{l|c| c|c|c|c|c|c|c|c}
    \multicolumn{2}{c|}{Application} & glibc & \NM{} & TCM & TCM-NUMA & jemalloc & TBB & Scalloc & mimalloc \\ \hline
    \multirow{2}{*}{canneal} & Remote & 772 & 613 & 718   & 626 & 690 & 752 & 655 & 709\\ \cline{2-10}
    & TLB Misses &  & 975  &    &  &1876  & 1910 & 1836 & 1876 \\ \hline
    \multirow{2}{*}{dedup}  & Remote & 35.7 & 23.8 & 23.2 & 32.6 & & 29.9 & 28.4 & 23.7\\ \cline{2-10}
     & TLB &  & 21.6 &    &  & & 31.1 & 42.0  & 19.2\\ \hline
    \multirow{2}{*}{fluidanimate} & Remote  & 784 & 145 & 755 & 902 & & 741 & 798 & 749\\ \cline{2-10}
    5.2
   & TLB  &  & 11.9  &    &  & & 116 & 137 & 100\\ \hline
    \multirow{2}{*}{streamcluster} & Remote  & 762 & 571 & 602 & 548 & & 768 & 497 & 570\\ \cline{2-10}
    &  TLB  &  & 12.1 &    &  & & 1455 & 1410 &  1432\\ \hline
    \multirow{2}{*}{pbzip2} & Remote  &  59.3 & 15.2 & 102 & 98.8 & & 62.5 & 45.8 & 60.4\\ \cline{2-10}
    & TLB &  & 0.8 &    &  & & 98.1 & 66.8 & 26.7 \\ \hline
    \end{tabular}
    \caption{TLB misses when using different allocators. The data shown is mega. }
    \label{tab:characteristics}
\end{table}

\end{comment}

%Table~\ref{tab:characteristics} shows that \NM{} significantly reduces the number of remote accesses and TLB misses due to its region-based design. 

%\todo{Hanmei: maybe we need to get the data on these applications. perf stat -e node-loads, node-load-misses, node-stores, node-store-misses ./APP a.out}



%We  can see that the average value of \NM{} is 0.97 in Machine A and 0.92 in Machine B and it is always the best among all other allocators. The reason that \NM{} got better performance in Machine B is that there are more nodes and more cores in Machine B, which means \NM{} could be very helpful to better to take use hardware resource of multi nodes and cores. but we could get amazing improvement if we shutdown interleaved heap in \NM{} and we will give the data in following sections.In the figure ~\ref{8node-parsec-perf}, we could see more exciting improvement from \NM{}, with average normalized value of 0.92 that is not only the best but also far aware better than all the rest allocators that TCMalloc and jemalloc got 0.99, TCMalloc-NUMA and TBB got roughly 1.07 and 1.01 separately. And also, we can see that the performance of \NM{} is the best for almost each single applications, especially it got 0.17 in fluidanimate and 0.66 in streamcluster which is far better than any of other allocators. As the same thing, the performance of ratrace and canneal is not good here, we will talk about it later after we shut down the interleaved heap.


%In the figure ~\ref{hoard-perf}, we show the normalized performance for Hoard benchmarks in Machine A and Machine B separately. We can see from figure ~\ref{hoard-perf} that the average value of \NM{} is also the best, which is 0.47 that means 2 times faster than default Linux Allocator, and jemalloc got 0.7 and Scalloc got 0.9. In the threadtest, the normalized value of \NM{} is 0.19 , far better than any of others, which means there are few central free list competitions, mainly contributed by properly node management and low overheads operations. For false sharing, \NM{}'s performance is also almost the best as same as Scalloc and jemalloc, which means they could handle false sharing issues very properly. In the larson, \NM{} and TCMalloc are the best, which mainly contributed by their low overheads for allocation and remote de-allocation, but due to our better node management, \NM{} could be better in the Machine B which will be mentioned later. In the figure ~\ref{hoard-perf}, we can also see that \NM{} got lowest average normalized value:0.33, significantly smaller than any of others that TBB got 0.99, Scalloc and jemalloc got roughly 1.14. And also, \NM{} and Scalloc could handle false sharing issue very well, and \NM{} could extremely well reduce central free list competition in threadtest. In larson, \NM{} is the best due to its properly multi-node management. 

\subsection{Memory Consumption}
\label{sec:memory}

We also measure the memory consumption of different allocators on PARSEC benchmark~\cite{parsec} and real applications, as shown in Table~\ref{tab:memory_consumption}.
%For non-server applications, such as \texttt{Aget}, \texttt{Pbscanf}, \texttt{Pbzip2} and all PARSEC applications, we utilized the sum of the \texttt{maxresident} output from the \texttt{time} utility and the size of huge pages, since the \texttt{time }output does not include huge pages. 
%To determine the size of huge pages, a script is used to periodically collect the number of huge pages by reading from \texttt{/proc/meminfo} file, and then the maximum value of huge pages is used. \todo{removed. This is for mmap's huge page, THP usage is counted}
%For server applications, such as \texttt{MySQL}, \texttt{SQLite}, and \texttt{Memcached}, \texttt{Apache}, the maximum memory is collected by the sum of both \texttt{VmHWM} and \texttt{HugetlbPages} fields from \texttt{/proc/PID/status} file, after the corresponding client exits. 
%We always reboot server applications for each single test. 
% Memory overhead is listed in Table~\ref{tab:memory_consumption}.
Overall, the default Linux allocator has the smallest memory consumption, and Intel TBB is the second-best one. \NM{}'s total memory consumption is around 17.6\% more than that of the default Linux allocator, but it is similar to TCMalloc and $1.63\times$ better than the second fastest allocator (mimalloc), as shown in Figure~\ref{fig:perf1}. 

The memory consumption of \NM{} is almost $4.5\times$ lower than that of Scalloc with huge page support. Similar to \NM{}, Scalloc allocates a big region of virtual memory from the underlying OS initially, which will be backed by huge pages physically. While the huge pages can improve the performance of an application, they can also lead to a significant increase in memory usage. For example, an application will use 2MB of physical memory even if it only allocates a small object (e.g., 8 bytes). Compared to Scalloc, \NM{} makes threads (with different size classes) running on the same node share the same huge page, as described in Section~\ref{sec:hugepages}, which effectively reduces its memory consumption. That is why the total memory consumption of \NM{} is far better than Scalloc. Interestingly, we observe that the memory consumption of \NM{} is similar to that of TCMalloc (TEMERAIRE~\cite{TEMERAIRE}), which is a state-of-the-art allocator optimized specifically for huge pages. We expect that \NM{}'s memory consumption can be further reduced by utilizing some complicated mechanisms proposed by TEMERAIRE.  


% As shown in the column of ``w/o THP'' of Table~\ref{tab:memory_consumption}, when the transparent huge page support is disabled, \NM{}'s memory overhead is actually comparable to Glibc and TBB, where the total memory consumption is decreased from 15938 MB to 13364 MB. That is, \NM{} imposes a similar memory overhead when not using huge pages. 


% The memory consumption of \NM{} is almost $4.5\times$ lower than Scalloc with huge page support. Similarly, Scalloc allocates a big region of virtual memory from the underlying OS initially, which will be backed by huge pages physically. \NM{}'s memory consumption is much lower due to its ``incremental sharing'' mechanism. Comparing to Scalloc, \NM{} makes all threads (with different size classes) in the same node share the same huge page, as described in Section~\ref{sec:hugepages}, which effectively reduces its memory consumption. 

%In total, Scalloc utilizes $7\times$ more memory when transparent huge page support is enabled.  

We further confirmed the memory consumption when transparent huge page support is disabled, which can be seen as the ``w/o THP'' column in Table~\ref{tab:memory_consumption}. In this case, \NM{}'s total memory overhead is actually comparable to the default Linux allocator and TBB, where the total memory consumption is decreased from 15938 MB to 13622 MB. That is, \NM{} imposes a low memory overhead when not using huge pages. 

% Other allocators will not be affected much by transparent huge pages, since they typically obtain a small chunk from the OS each time, less than the size of a huge page (2MB), then the OS will not allocate physical pages from huge pages by default. Therefore, we believe that \NM{}'s memory consumption is acceptable. 

%Second, \NM{} may not return the memory to the OS immediately for large pages. However, we believe that its memory consumption is acceptable. 
  %which actually shows the worst case for \NM{}. The OS will utilize huge pages if a memory area is larger than the size of a huge page (2MB). Since \NM{} utilizes \texttt{mmap} to allocate a huge chunk of virtual memory, this makes all heap memory for real objects will be allocated from huge pages. Currently, \NM{} also utilizes 1MB as the superblock for each size class, making objects of a size class that will occupy at least 1MB even if it only uses an object inside. 
 % Therefore, an application with many size classes will waste more memory. \NM{} makes all threads share the same bag for each size class, as described in Section~\ref{sec: others}, which effectively reduces its memory consumption by multiple times. 
 
 %Scalloc has excessive memory consumption, since its design does not support transparent huge pages very well. Similar to \NM{}, Scalloc utilizes a \texttt{mmap} system call to allocate a continuous huge region of virtual memory from the underlying OS. Since every thread will get a virtual span (2MB) for each size class in Scalloc, it will utilize 2MB physical memory even if only a word is touched. Differently, \NM{} avoids this issue as described in Section~\ref{sec: others}.
% the OS will assign a huge page when transparent huge page is enabled by default. Thus, if only one object is allocated from a size class, 
 
\begin{comment}


In Figure~\ref{2node-hoard-mem}, the average normalized value of \NM{} is larger than others, but actually not too much, which is 2.3 for \NM{}, 1.9 for TCMalloc-NUMA and 1.8 for TCMalloc. It is because that proper node management is utilized in \NM{} and also in TCMalloc-NUMA, so that each node also preserves some memory not only thread locals. But we believe that these little more memory overheads are totally acceptable. It is also the same thing for Figure 10, that the average value for \NM{} is a little higher than others, which is 5.3. But in this 8 nodes machine, \NM{} is not the worst, that Scalloc's average value is 25 and jemalloc is 9.4. One main reason that the value of \NM{} is smaller is that we use mini-size bags in \NM{} which is less than the size of one page for small objects and also memories for small objects are shared per node but per cores in Scalloc.
	
\end{comment}

\begin{figure*}[!th]
    \centering
    \includegraphics[width=6.1in]{figure/sythentic-scalobility-new.pdf}
    \caption{Scalability evaluation of different allocators.\\ All data are normalized to the runtime of the default Linux allocator with one thread.}
    \label{sythentic-scalability}
\end{figure*}

\begin{figure}[!h]
\centering
\subfloat[Normalized runtime without thread binding for Glibc and TCMalloc, where the lower is the better.]{
  \includegraphics[width=3.2in]{figure/threadbinding1.jpg}
}

\subfloat[Normalized runtime with node-interleaved and node-saturate binding for \NM{}, where the lower is the better.]{
  \includegraphics[width=3.2in]{figure/threadbinding2.jpg}
}
\caption{Performance impact of thread binding.}
\label{binding-pthread-scalibity}
\end{figure}

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=3.2in]{figure/interleavedheap.jpg}
%     \caption{Normalized runtime with and without the interleaved heap for \NM{}.  \label{fig:interleavedheap}}
% \end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=3.2in]{figure/origin-aware.jpg}
    \caption{Normalized runtime with and without origin-aware deallocation for \NM{}.  \label{fig:origin}}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=3.2in]{figure/hugepage.jpg}
    \caption{Normalized runtime with and without THP for \NM{}.  \label{fig:hugepage}}
\end{figure}

\subsection{Scalability}
\label{sec:scale}

%We also evaluated the scalability of different allocators. 

To validate the scalability of \NM{}, we use four synthetic applications from Hoard~\cite{Hoard}, including \texttt{threadtest}, \texttt{larson}~\cite{Larson}, \texttt{cache-scratch} and \texttt{cache-slash}, which is also employed by existing work~\cite{Scalloc}. We do not use the PARSEC applications in Section~\ref{sec:evaluation}, as they are not scalable by design.
%We have verified blacksholes, bodytrack, canneal, and raytrace.
For instance, raytrace has no performance difference when running with 16 threads or 40 threads.
%Therefore, we are using four synthetic applications from Hoard~\cite{Hoard}, including \texttt{threadtest}, \texttt{larson}~\cite{Larson}, \texttt{cache-scratch} and \texttt{cache-slash}, which is also employed by existing work~\cite{Scalloc}. \texttt{larson} simulates a multithreaded server that could respond to requests from different clients, and \texttt{threadtest} is an application that performs a large number of allocations and deallocations within a specified number of threads. Both \texttt{cache-scratch} and \texttt{cache-thrash} test false sharing issues that can be introduced by allocators, where multiple threads are getting and accessing different objects in the same cache line.  
%Passive false sharing is introduced upon deallocations, where a freed object can be utilized by another thread. In contrast, active false sharing is introduced during the initial allocations, where multiple continuous objects sharing the same cache line are allocated to different threads. The synthetic applications have a better scalability by design than other evaluated applications in the last section. 
%Since other allocators cannot specify the configuration, we only evaluate the scalability with different number of threads. 
In the evaluation, we maximize the number of threads on each node for \NM{}. For instance, 32 threads will use 2 nodes, as each node has 16 cores. For other allocators, we only specify the number of threads, and it is up to the OS to determine the scheduling. 
%The corresponding data is shown in Figure\ref{sythentic-scalability}. All data are normalized to the data of one thread of the Linux's default allocator, where the higher is better.
 
Figure~\ref{sythentic-scalability} illustrates different allocators' performance speedup with the increasing number of threads. All data are normalized to the runtime of Linux's default allocator under one thread. Overall, \NM{} has the best performance when the number of threads is 128. Its average speedup is $88\times$, compared to Linux's allocator with one thread, while the second-best allocator -- \texttt{mimalloc} -- has a $75\times$ speedup. In contrast, the default Linux allocator only has a speedup of $49\times$. That is, \NM{} has the best scalability compared to other allocators.
%When computing the speedup using the data of one thread of each allocator, \NM{}'s average speedup is $65\times$, while the second-best one is $54\times$. 

Among these applications, both \texttt{cache-scratch} and \texttt{cache-thrash} test false sharing issues that can be introduced by allocators, where multiple threads access different objects in the same cache line. When false sharing occurs, threads accessing seemingly unrelated data will invalidate each other when performing writes, resulting in performance degradation.
\texttt{cache-scratch} tests passive false sharing, which is introduced upon deallocations, where a freed object can be utilized by another thread. \texttt{cache-thrash} tests active false sharing, which in contrast is introduced during the initial allocations, where multiple continuous objects sharing the same cache line are allocated to different threads.
% \texttt{cache-scratch} tests passive false sharing, and \texttt{cache-thrash} tests active false sharing. Passive false sharing is introduced upon deallocations, where a freed object can be utilized by another thread. In contrast, active false sharing is introduced during the initial allocations, where multiple continuous objects sharing the same cache line are allocated to different threads.
Based on our understanding, \NM{} will not introduce active false sharing, since each thread will get a page of objects initially. Although \NM{} might introduce some passive false sharing due to its per-thread cache design, it avoids remote allocations across the node, where other allocators do not have such mechanisms. We believe that is the major reason for \NM{} better performance.
% For both applications, \NM{} has similar scalability compared to other allocators up to 64 threads. But when it comes to 128 threads, \NM{} outperforms all other allocators. 
% According to our investigation, other allocators suffer from false sharing issues while \NM{} alleviates the problem and thus has better performance.


In these four applications, \NM{} only performs worse than mimalloc for \texttt{larson} with 128 threads. \texttt{larson} simulates a multithreaded server that can respond to requests from different clients. In this application, each thread is given a set of objects, and they perform random deallocations and allocations on these objects within a round, and finally pass the objects to the next thread before terminating. Unlike other applications, \texttt{larson} runs for a fixed time and we use a throughput metric (the number of memory allocations per second) to measure the performance. Remote deallocations are quite common for this application, as local objects can be passed to other remote threads. Therefore, the performance of \texttt{larson} is sensitive to the memory recycling mechanisms of the allocator, as observed in the existing work~\cite{Scalloc, scalableallocator}. As discussed in Section~\ref{sec:origin}, the remote object's deallocation is managed by the original node's freelist to ensure the locality. This freelist is shared among all threads running on this node, which can become a bottleneck when serving multiple deallocations at the same time. Nevertheless, \NM{} still performs better than most allocators on \texttt{larson} and can scale to 128 threads.
% Although most deallocations are performed by the allocating thread which occurs on the local node, remote deallocations are also common, as local objects can be passed to other remote threads. Therefore, } 
% we do not know the exact reason for this, but \NM{} is still good enough against other allocators. \NEW{reason!}
% but it performs better against mimalloc for other applications. 
All of these data indicate that \NM{} is scalable to 128 cores. 
%We don't know the exact reason for this.

% \todo{the scalability disadvantage of NUMAlloc at 32 or 64 threads, or > 128 threads}

% \todo{Previous we have analysis of TCMalloc on two cache applications, but I think it's not representative as larson and thread-test}

%Among these applications, \texttt{cache-scratch} tests passive false sharing, and \texttt{cache-thrash} tests active false sharing. False sharing occurs when multiple threads are concurrently accessing different words in the same cache line. 
%Passive false sharing is introduced upon deallocations, where a freed object can be utilized by another thread. In contrast, active false sharing is introduced during the initial allocations, where multiple continuous objects sharing the same cache line are allocated to different threads. 
%For these false sharing tests, we use 100,000 inner-loop, and 100,000 iterations with 8-byte objects.

\begin{comment}
Among these applications, \texttt{cache-scratch} tests passive false sharing, and \texttt{cache-thrash} tests active false sharing. TCMalloc has serious issues of both active and passive false sharing issues, which is the major reason that it does not perform well on both applications. 
\NM{} will not introduce active false sharing, since each thread will get a page of objects initially. Although \NM{} might introduce passive false sharing due to its per-thread cache design, it avoids remote allocations across the node. 
%We believe that is the major reason for its better performance. Other allocators do not have such mechanisms. That is the reason why 
\NM{} is one of the best allocators for \texttt{cache-scratch}, and achieves much better speedup than all other allocators in \texttt{cache-thrash} (30\% faster than the second-best one), as shown in Figure~\ref{sythentic-scalability}. 
\end{comment} 
 %\texttt{larson} is to simulate a multithreaded server that could respond to requests from different clients. \NM{} is around 16\% faster than the second-best allocator--TCMalloc.   \texttt{threadtest} is an application that performs a large number of allocations and deallocations within a specified number of threads. \NM{} is $2.6\times$ faster than the second-best one (jemalloc), when there are 128 threads.
 %Each thread will receive a random number of objects in the beginning, perform a random number of allocation and deallocations to simulate the handler for processing requests, and then pass objects to the next thread. We test \texttt{larson} for 10 seconds with 1,000 objects for 10,000 iterations, where each allocation is between 7 bytes and 2048 bytes. 
 %As shown in Figure~\ref{sythentic-scalability},  



%Also, it allows to specify how much work to be done between each allocation and deallocation. For \texttt{threadtest}, we use 100 iterations, 1,280,000 allocations, 0 work, and 64-byte objects (for the allocation).  This benchmark will stressfully test the performance overhead of allocation and deallocation. For this application, 
 %since every thread will has its own heap and it only imposes some when getting objects from the shared bag. But \NM{} obtains a number of objects at a time, at the page level, which significant reduce the possibility of contention. 


%On average,  \NM{} is running 79\% faster than the second-best one (Scalloc), and $2.2\times$ faster than the default Linux allocator. Multiple reasons contribute to the good performance of \NM{}: \NM{} imposes very minimal system call overhead, and little synchronization overhead. Also, it introduces less remote accesses than all other allocators, due to its NUMA-aware design. Other allocators have more or less false sharing issue, or incurs remote accesses.  
%\todo{Surprisingly, TCMalloc and  }.



%That is the reason why it has a good performance as the Linux allocator for \texttt{cache-thrash}.

 
\begin{comment}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=5.5in]{figure/scalobility-pthread.pdf}
    \caption{Normalized performance of Linux's default allocator without binding for PARSEC benchmarks in Machine B}
    \label{pthread-scalibity}
\end{figure}

We will evaluate the scalability on 8threads, 16threads, 32 threads, 64 threads and 128 threads. 
(one node, two node, four nodes, and 8 nodes). 
	
\end{comment}


\subsection{Design Choices}
\label{sec:design}

This section further confirms \NM{}'s multiple design choices.
% all results shown in this section are normalized to the data of the default Linux allocator.  

%Based on our analysis, there are two reasons for this performance speedup. First, a thread will not be migrated to a different core, avoiding unnecessary remote accesses caused by cross-node migration, as further discussed in Section~\ref{sec:intro}. Second, \NM{}'s thread binding balances the workload, thus reducing the congestion of interconnect or one memory controller.
\subsubsection{Choices of Thread Binding}
\label{sec: threadbinding}

% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width=6in]{figure/WO-pthread-binding3.pdf}
%     \caption{Normalized runtime with and without node-balanced thread binding for Glibc and TCMalloc.} 
%     \label{binding-pthread-scalibity}
% \end{figure*}

% \begin{figure*}[!ht]
%     \centering
%     \includegraphics[width=5.5in]{figure/binding-policy.pdf}
%     \caption{ Node-Interleaved vs. Node-Saturate binding for Glibc, where results are normalized to those without binding.}
%     \label{fig:binding-policy}  
% \end{figure*}

\NM{}'s memory management is based on binding, including thread binding and memory binding. We believe such bindings benefit the performance and open up other design opportunities, such as origin-aware memory management, metadata allocation and incremental sharing. The combination of all design choices makes \NM{} a faster and more efficient allocator. Therefore, we cannot evaluate the impact of thread binding by directly disabling it on \NM{} since other designs depend on it. To overcome this problem, we implement a thread binding library that allows other allocators to enable binding.
Figure~\ref{binding-pthread-scalibity}(a) shows the impact of thread binding on two allocators, Glibc and TCMalloc.
% Figure\ref{binding-pthread-scalibity}(a) shows the performance difference without thread binding for two allocators, Glibc and TCMalloc.
The results are normalized to the data with thread binding of each allocator, respectively, so we omit the ones with thread binding. Thus, this figure can be considered to show how much slower it would run without thread binding.
% We cannot evaluate \NM{} directly, since its mechanisms are tightened to thread binding, such as incremental sharing, origin-aware memory management and metadata allocation. 
Here we use the node-interleaved binding. As shown in Figure~\ref{binding-pthread-scalibity}(a), the thread binding improves the performance significantly for some applications. For instance, \texttt{fluidanimate} runs around $4.45\times$ faster on Glibc and $3.66\times$ faster on TCMalloc with the node-interleaved thread binding. Similarly, \texttt{streamcluster} runs around 20\% and 30\% faster than the corresponding one without the binding. 
We further use \texttt{perf}~\cite{perfweb} to analyze the reasons for the significant performance improvement of these two applications. The results confirm that remote accesses are significantly reduced with thread binding, mainly due to the elimination of thread migration. Interestingly, the cache miss rate also decreases with thread binding. Overall, thread binding will benefit the performance of most applications without hurting others,
% This clearly indicates that the thread binding will benefit the performance overall
which should be included in the memory allocator by default. 

% In particular, threads are bound to different nodes in a round-bin way, called node-balanced binding, which is the same as \NM{}.

We also compare two types of thread binding: node-interleaved and node-saturate thread binding. In node-saturate binding, we bind the maximum possible number of threads (same as the number of cores) to a node and then switch to the next node. As shown in Figure~\ref{binding-pthread-scalibity}(b), the node-interleaved thread binding is almost always better than node-saturate thread binding, except for \texttt{vips}. On average, node-interleaved binding is around 19\% faster than node-saturate one for these evaluated applications. This indicates that people should use node-interleaved binding, if they would like to employ all hardware cores. However, if they only want to use partial cores, then the node-saturate binding could be a better choice. Furthermore, \NM{} allows users to adjust the binding option according to their requirements.

% , proving the effectiveness of Node-Balanced thread binding.

\subsubsection{Impact of Origin-aware Deallocation}

As discussed in \ref{sec:origin}, \NM{} adopts an origin-aware memory management to ensure the locality of memory allocations and deallocations. Some NUMA-aware allocators take locality into consideration during allocation, but neglect to handle the remote deallocation, resulting in remote accesses when reusing the memory. Instead, \NM{} proposes origin-aware deallocation which guarantees that a freed object will always return to its original node's heap. We further verified the effect of this design and the results are shown in Figure~\ref{fig:origin}, where the data is normalized to the runtime with origin-aware deallocation. According to the Figure~\ref{fig:origin}, all evaluated applications benefit from origin-aware deallocation and applications that have more remote deallocations, such as canneal, streamcluster and vips, achieve significant performance improvements. Overall, \NM{} runs 3.8\% slower if we don't consider the origin of freed objects.


\subsubsection{Impact of Transparent Huge Pages}

As discussed in \ref{sec:hugepages}, it's beneficial to embrace the transparent huge page support in modern systems. We evaluate the performance impact of transparent huge pages. The results are shown in Figure~\ref{fig:hugepage}. When integrating with transparent huge pages, \NM{} achieves significantly better performance for \texttt{vips}, where it is running 16\% faster. On average, transparent huge pages improve the performance by about 2.62\%. There are no applications that run slower with huge pages. This clearly indicates that it is beneficial to enable transparent huge pages for the NUMA architecture, especially when \NM{} is used. 
Although \NM{} may increase the memory overhead 
% from 2\% to 10\% 
when using huge pages, as shown in Table~\ref{tab:memory_consumption}, the memory overhead is still acceptable, given the comparison of other mainstream allocators and the hardware trend of increasing memory capacity.


\begin{comment}

\subsubsection{Impact of Interleaved Heap} 
\label{sec:interleavedheap}

We also evaluate the potential benefit of the interleaved heap. The performance data is shown in Figure\ref{fig:interleavedheap}. 
% Note that we have evaluated all PARSEC applications listed in Section~\ref{sec:performance}.
Based on the figure, we have the following conclusion: the interleaved heap will benefit (or at least has no harmful impact on) the performance for most applications. In particular, it improves the performance significantly on \texttt{fluidanimate} and \texttt{streamcluster}.  However, applications having a large portion of time spent in the serial phase, such as \texttt{canneal} and \texttt{raytrace}, may hurt the performance with the interleaved heap support. These two applications share the same property that they have a larger portion of the first serial phase. With the interleaved heap, \NM{} allocates the memory from different nodes interleavedly for the serial phase, instead of from the local node (based on the default first-touch policy). That is, some private objects allocated in a remote node may introduce unnecessary performance overhead due to remote accesses. Currently, we do not know why swaptions has a large negative performance impact when the interleaved heap is employed, whose performance can change dramatically when adding one meaningless code line (declaration and assignment of a \texttt{volatile} variable) or changing the compilation flags. We will keep investigating this. 
%Further, upon each allocation, \NM{} checks the callstack to confirm whether an allocation is from a potentially shared heap, which also introduces some overhead. Therefore, the interleaved heap will be enabled by default, unless programmers know that it will not benefit the performance.

Programmers can choose to enable or disable interleaved heap based on the applications. A simple metric is to use the portion of the serial phase inside multithreaded applications. For applications that are mostly running in the serial phase, turning off the interleaved heap support may be a better choice. That is, the interleaved heap will harm the serial execution, but may benefit the parallel execution because of its load balance. It is easy to turn on/off the interleaved heap via a compilation flag or the environment variable. 


\end{comment}


% \todo{other design choices: THP's impact on performance, origin-based deallocation, autonuma's impact...}

%, except applications with a large portion of serial phase (e.g., \texttt{canneal} and \texttt{raytrace})

%The interleaved heap could be utilized to avoid load imbalance issue for shared objets. 

%However, there are two issues for the interleaved heap. First, the allocator may not know whether an object is shared or not at the first time. Therefore, all objects that are allocated in the main heap (before creating any child thread) will be treated as the shared heap. Second, some applications are spending too much time in the serial phase, where the interleaved heap cannot benefit the performance for the serial phase. 





%figure ~\ref{parsec-no-interleaved-perf} we show some performance results of some applications that got significant different values after we shut down interleaved heap for \NM{}. We can see that for some applicatios with less data sharing between threads like ratrace and canneal, \NM{} could got significant improvements due to its low overheads and proper memory management. But for some other applications with intensive memory operations and sharing like fluidanimate, shutting down interleaved heap could hurt performance, since interleaved heap could help to distributed resource contention evenly over multi-nodes and then got low overheads.

%\subsubsection{Selective Huge Pages} 
%\label{sec:hugepage}

%Since the machine utilizes transparent huge pages by default, we evaluate the performance impact of huge page support on another machine with 2 NUMA-node, without enabling the transparent huge pages.
% A, the 2-node machine. We only utilize PARSEC applications for this evaluation. 

%\begin{figure}[!h]
%    \centering
%    \includegraphics[width=3.2in]{figure/hugepage.pdf}
%    \caption{Normalized runtime with and without selective huge pages.}
%    \label{fig:hugepage}
%\end{figure}

%The results are shown in Figure~\ref{fig:hugepage}. When integrating with selective huge pages, \NM{} achieves a significantly better performance for \texttt{dedup}, where the performance difference is around 15\%. On average, the selective huge pages improves the performance of all evaluated applications about 2.5\%, and will not hurt the performance. This clearly indicates that it is beneficial to have selective huge pages for the NUMA architecture, especially given the fact of increasing memory size of hardware trend.  


