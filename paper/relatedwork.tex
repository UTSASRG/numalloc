\section{Related Work}

\label{sec:related}

This section discusses some related work with \NM{}. 

\paragraph{General Purpose Allocators:}
 There exists a large number of allocators~\cite{dlmalloc, Hoard, TCMalloc, jemalloc, Scalloc}, but they are not designed for NUMA architecture. Based on the management of small objects, allocators can be further classified into multiple types, such as sequential, BiBOP, and region-based allocators~\cite{DieHarder, Gay:1998:MME:277650.277748}. Region-based allocators are suitable for special situations where all allocated objects within the same region can be deallocated at once~\cite{Gay:1998:MME:277650.277748}. For sequential allocators, subsequent memory allocations are satisfied in the continuous memory area, such as the Linux allocator (originating from dlmalloc~\cite{dlmalloc}) and Windows allocator~\cite{DieHarder}. For such allocators, objects with different sizes can be placed continuously. For BiBOP-style allocators, one or multiple continuous pages are treated as a ``bag'', holding objects with the same size class. %The metadata of heap objects, such as its size and availability information, is typically stored in a separate area. Thus, BIBOP-style allocators improve the security and reliability, by avoiding metadata corruption caused by buffer overflows. 
 Many performance-oriented allocators, such as TCMalloc~\cite{TCMalloc}, \texttt{jemalloc}~\cite{jemalloc}, Hord~\cite{Hoard}, Scalloc~\cite{Scalloc}, and most secure allocators, such as OpenBSD~\cite{OpenBSD} and DieHarder~\cite{DieHarder}, belong to this type.  \NM{} belongs to BiBOP-style allocators, and proposes multiple designs for the NUMA architecture. 


Scalloc utilizes the same-sized spans in order to encourage memory reuses~\cite{Scalloc}. Salloc has the other two contributions, a global backend developed by concurrent data structures, and a constant-time front end that returns the spans to the backend. 
% 
%. First, they typically manage objects in multiple size classes, instead of using the exact size. Second, they typically utilize different mechanisms to manage small objects and large objects. For instance, small objects may utilize freelists to track freed objects upon deallocations, but large objects may be returned to the OS directly.  


%For sequential allocators, subsequent memory allocations are satisfied in the continuous memory area, such as the Linux allocator (originating from dlmalloc~\cite{dlmalloc}) and Windows allocator~\cite{DieHarder}. After an allocation, the pointer is bumped to the end of the current object to satisfy the subsequent allocations, which is also called as \texttt{bump allocators}. 
%For such allocators, objects with different sizes can be placed continuously. For BiBOP-style allocators, one or multiple continuous pages are treated as a ``bag'', holding objects with the same size class. The metadata of heap objects, such as its size and availability information, is typically stored in a separate area. Thus, BIBOP-style allocators improve the security and reliability, by avoiding metadata corruption caused by buffer overflows. Many performance-oriented allocators, such as TCMalloc~\cite{TCMalloc}, \texttt{jemalloc}~\cite{jemalloc}, Hord~\cite{Hoard}, Scalloc~\cite{Scalloc}, and most secure allocators, such as OpenBSD~\cite{OpenBSD} and DieHarder~\cite{DieHarder}, belong to this type.  \NM{} belongs to BiBOP-style allocators, and proposes multiple designs for the NUMA architecture. 

%Upon deallocations, freed objects are typically placed into different free lists based on their size classes. In order to get the size information, these allocators typically place the size information just before actual objects. 

%BiBOP stands for ''Big Bag of Pages''~\cite{hanson1980}. BiBOP allocators may utilize free lists or bitmaps to manage the availability of objects. 


%~\cite{jemalloc} introduces a round-robin fashion for arena allocation, and all locks and buffers will be local to the associated arena. Similarly, \NM{} also takes the similar approach. 



%Bolosky et. al. propose a simple mechanism to improve the performance by replicating read-only pages to multiple processors, moving pages to the processor that written them, and placing pages to the global memory if they are written by multiple processes ~\cite{Bolosky:1989:SBE:74850.74854}.

\paragraph{NUMA-aware Allocators:} There exists some allocators for the NUMA architecture. 
Ogasawara et. al. focuses on finding the preferred node location for JAVA objects during the garbage collection and memory allocations~\cite{Ogasawara:2009:NMM:1640089.1640117}, via thread stack, synchronization information, and object reference graph. The proposed method is not suitable for C/C++ applications, since the objects were tighten to physical pages. 
Memarzia et. al. proposes a NUMA-aware system that  focuses on improving the query performance of in-memory databases~\cite{wagle2015numa}. It confirms that dynamic memory allocations, thread placement and scheduling, memory placement policies, OS configurations as page size and load balance may have a direct impact on the performance of NUMA systems. However, it is not designed as a new memory allocators, which has a different goal as \NM{}. TcMalloc-NUMA adds additional node-based freelists and free spans to store freed objects and pages belonging to the same node~\cite{tcmallocnew}, which is similar to \NM{}. It also invokes the \texttt{mbind} system call to bind physical memory allocations to the node that the current thread is running on. However, it does not support huge pages and interleaved, invokes too many \texttt{mbind} system calls, and does not handle the metadata's locality. 

nMART proposes a NUMA-aware memory allocation for soft real-time system~\cite{kim2013node}. It proposes node-oriented allocation policy to minimize the access latency, and ensures temporal and spatial guarantee for real-time systems. nMART requires the change of the underlying OS, which is different from \NM{}. nMART also has a different target as \NM{} that tries to meet the time requirement of real-time systems, and \NM{} focuses more on the performance. nMART has node-based arena, which is similar to \NM{}'s per-node freelist. 

 



%However, there are multiple issues of this approach. It always assumes that the physical memory will be always allocated in the same node as the thread who requests the memory. It is not true due to the load-balance inside the OS, and some threads may allocate some objects that are exclusively used by other threads. Second, it did not handle the metadata of different nodes. Third, it does not bind threads to different nodes, which may cause large amount of remote accesses when a thread is scheduled to a different core. 


\paragraph{Other NUMA Relatd Systems:} 
Majo et. al. proposes to consider both data locality and cache contention, and combine memory management with task scheduling to achieve better performance~\cite{Majo:2011:MMN:1993478.1993481}. Mostly, it focuses on the task scheduling.  Majo et. al. proposes to set task-to-thread affinity, and pin threads to specific cores to achieve a better performance~\cite{Majo:2015:LPC:2688500.2688509}. It is not a memory management policy, and mainly talks about the detail implementation of task binding over different levels of the TBB library. NumaGiC reduces remote accesses in garbage collection phases with a mostly-distributed design that each GC thread will mostly collect memory references locally, and utilize a work-stealing mode only when no local references are available~\cite{NumaGiC}.  Diener proposes to combine the task management and memory management to achieve the better performance for a new kernel framework~\cite{diener2015automatic}. For task mapping, it proposes to place tasks that shared the data onto the cores that share the cache, in order to reduce the cache misses and the communication overhead. For memory management, it utilizes the page faults to analyze the memory access behavior, and then migrate pages between nodes to improve the performance. However, this work is not about a new memory allocator that combines both together, which is different from \NM{}. 

%It requires the changes of the underlying OS, and will impose some overhead of understanding the communicating tasks and memory access behavior via analyzing page faults.  

 
~\cite{1419934}
Kaminski et al. proposes to make TCMalloc NUMA-aware~\cite{tcmallocnew}, with the very minimum effort. The idea is to maximize the local memory allocation with the node-based free list and page heap. However, this mechanism assumes that the memory deallocation from the current node will be always allocated from the local node. This is unfortunately not true for many cases, such as a producer-consumer model. Also, this allocator does not take advantage of 


 
%We describe two scheduling algorithms: maximum-local, which optimizes for maximum data locality, and its extension, N-MASS, which reduces data locality to avoid the performance degradation caused by cache contention. N-MASS is fine-tuned to support memory management on NUMA-multicores and improves performance up to 32\%, and 7\% on average, over the default setup in current Linux implementations.

%~\cite{memarzia2019toward} investigates the performance impact of utilizing different memory allocators, different task assignment, different memory allocation policies, and different OS configuration. It showed that some specific allocator could improve the performance up to $20\times$ when using the interleaved memory allocation policy and modify the OS configuration. 

% http://memkind.github.io/memkind/ 
% Use this to compare the performance difference
% However, this will require the change of applications to use their library. 
~\cite{cantalupo2015memkind} proposes a new library that allows users to manage their memory in fine granularity by combining with multiple existing system calls. However, they are not targeting for a general purpose allocator, since it requires programmers to manage the memory explicitly. Unfortunately, this place unacceptable burden to programmers. More importantly, the explicit management based on one existing topology may not work well for the hardware with a different topology. 



\paragraph{Other NUMA-related Systems:} 

Memory system performance in a numa multicore multiprocessor

~\cite{Majo:2015:LPC:2688500.2688509} proposes TBB-NUMA, a system that mainly focuses on task scheduling on NUMA architecture to achieve better performance. \NM{} employs a similar mechanism as TBB-NUMA to manage threads explicitly, but without relying on the human hints. \NM{} also deals with the memory allocation that is not presented in TBB-NUMA. 

~\cite{6704666} shows that a set of simple algorithmic changes coupled with commonly available OS functionality suffice to eliminate data sharing and to regularize the memory access patterns for a subset of the PARSEC parallel benchmarks. These simple source-level changes result in performance improvements of up to 3.1X, but more importantly, they lead to a fairer and more accurate performance evaluation on NUMA-multicore systems. In the default
configuration, OSs (such as Linux) tend to change the thread-to-core mapping during the execution of programs, which result in large performance variations.

~\cite{Bui:2019:EPV:3302424.3303960} talks about the virtualization of NUMA.

 bl

