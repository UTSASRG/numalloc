
\section{Introduction}
\label{sec:intro}

The Non-Uniform Memory Access (NUMA) architecture is an appealing solution for the scalability of multi-core era. Compared to Uniform Memory Access (UMA) architecture, the NUMA architecture avoids the bottleneck of a central memory controller: each processor (also known as a domain or node) consists of multiple cores, while each node can access its own memory controller. Different nodes are connected via high-speed inter-connection (such as Quick Path Interconnect (QPI)~\cite{intelqpi} or HyperTransport bus~\cite{hypertransport}) to form a cache-coherent system that presents an abstraction of a single globally addressable memory. 
However, NUMA applications may have serious performance issues, if programs have one of the following issues, such as large amount of remote accesses, load imbalance, contention for interconnection and last level cache~\cite{Blagodurov:2011:CNC:2002181.2002182, Dashti:2013:TMH:2451116.2451157}, as discussed more in Section~\ref{sec:overview}. 

Multiple types of approaches help improve the performance of NUMA systems. The first type of approaches migrate tasks or physical pages reactively based on memory access patterns or other hardware characteristics~\cite{Blagodurov:2011:CNC:2002181.2002182, AutoNUMA, Dashti:2013:TMH:2451116.2451157, Lepers:2015:TMP:2813767.2813788}. They improve the performance automatically without human involvement. However, they cannot solve all issues. For instance, if multiple objects located in the same page are concurrently accessed by multiple threads, either task migration or page movement cannot eliminate remote accesses completely~\cite{Gaud:2014:LPM:2643634.2643659}. The second type of approaches rely on programmers to manage memory allocations and task assignments explicitly~\cite{Kaestle:2015:SSA:2813767.2813787, Lin:2016:MTP:2872362.2872401, Majo:2017:LPC:3057718.3040222}. Although they could improve the performance greatly, they typically require significant human effort to rewrite the programs, where legacy systems cannot benefit automatically. 

%However, these approaches are not architecture or OS independent, such as Carrefour~\cite{Dashti:2013:TMH:2451116.2451157}. More importantly, they cannot achieve the optimal performance due to \textbf{their reactive approaches} that only migrate a task or memory pages whenever contention. However, the migration  will require additional overhead. Also, some issues cannot be solved by the reactive approach. For instance,  


Another type of approach focus on heap management~\cite{tcmallocnew}, since the heap easily cause performance issue due to extensive allocations and deallocations and extensive accesses on these heap objects. The NUMA-aware memory management is appealing, because there is no need to change applications explicitly. For this task, it is essential to reduce the number of remote accesses. Kaminski et. al. designs a NUMA-aware memory management based on TcMalloc~\cite{tcmallocnew}, called as TcMalloc-NUMA in the remainder of this paper.TcMalloc-NUMA designs a node-aware memory management by tracking the memory locality information. It invokes the \texttt{mbind} system call to bind the memory explicitly, so that it could check the locality via the stored information inside the user space. Also, it could always return a freed object back to a corresponding node based on the locality information. However, TcMalloc-NUMA has multiple design issues that may affect its performance. First, it does not regulate the task assignment, which could potentially introduce significant overhead by querying the locality of tasks dynamically. Second, it does not consider the load balance issue, but only focusing on remote accesses. Third, it does not employ the recent progress of the hardware and OS support, such as huge pages, due to its development in 2012. 


\begin{comment}

It first assumes that a memory block is belong to the same node for its allocating thread. However, this assumption is invalid, and is also contradict with the first-touch policy of the OS. 

If there is no such assumption, we will expect that the OS will provide an efficient API to query the locality of a page. However, no such APIs exist in both Linux and Windows. 

It checks the physical memory usage to determine the future allocations of the same node or the re-use of a remote node. But that is very slow by checking meminfo. 

In the end, it utilizes the mbind to bind the memory to a node specifically. 
	
\end{comment}
	 

%Based on our evaluation, TcMalloc-NUMA cannot achieve good performance for many applications.  
%However, none of these systems could achieve the performance promised by the hardware. Given a simple example, if one page is fulled with objects that are utilized by different threads running on different nodes, then it is not able to achieve the best performance no matter where this page is located. Similarly, the passive tracking of memory ownership of an object cannot completely avoid the ownership drifting problem, where the . 

This paper proposes a novel memory manage --\NM{}--to address these issues. First, it combines memory management and task management altogether, by binding tasks to different NUMA nodes interleavedly and binding the memory explicitly. The task binding not only benefits the load balance, due to its even distribution of tasks, but also constructs the basis for its memory management. In this design, there is no need to query the locality of tasks dynamically, and it is guaranteed to have the correct locality for the memory. Second, it proposes an interleaved heap for potentially-shared objects that are typically allocated in the serial phase. This design helps solve the load imbalance issue, when the main thread performs a large number of allocations but then pass them to different threads.  Third, \NM{} also employs the recent progress of the hardware and the OS, such as huge pages support. Based on our evaluation, this design achieves additional 10\% performance speedup automatically for some applications, due to the reduced TLB misses and cache misses. Besides that, \NM{} also implements very carefully in order to achieve the good performance. It introduces many implementation techniques, such as fast metadata lookup, cache warmup mechanism, efficient objects migration, node-local metadata, as further described in Section~\ref{sec:implement}. 
 
% Second, it proactively bound the memory to different nodes, so that it could reduce the . Third, it takes a different method to deal with allocations from main thread, Fourth, it takes the advantage of hardware progress automatically, e.g., huge pages. One application achieves 10\% performance speedup with this consideration.  


%There are other implementation issues that may also contribute to the performance improvement. For instance, the metadata will be always allocated locally. The big objects will be re-utilized by small objects automatically, in order to reduce the potential cache misses. 

To evaluate \NM{}, we perform extensive evaluation on synthetic and real applications, and compare \NM{} with popular allocators, such as the default Linux allocator, TcMalloc~\cite{tcmalloc}, jemalloc~\cite{jemalloc}, Intel TBB~\cite{tbb}, and Scalloc~\cite{Scalloc}. For the performance on real applications, \NM{} achieves around 13\% speedup on average comparing to the default Linux allocator, which is also 11\% faster than the second best one. For the \texttt{fluidanimate} application, \NM{} achieves up to $5.8\times$ faster than the default allocator, and $4.7\times$ faster than the second best one. For synthetic applications, \NM{} is running 44\% faster than the second best one in an 2-node machine, and is around $3\times$ faster than the second best one in an 8-node machine.  In total, \NM{} may impose around \todo{XX} memory overhead, but is much more scalable than all other allocators.  \NM{} is ready for the practical employment, due to its high performance, and good scalability. 

Overall, this paper has the following contributions. 

\begin{itemize}
\item First, we propose to integrate task management into the memory allocation for the NUMA memory management. 
\item Second, we propose an interleaved heap in order to reduce the load balance issue caused by shared objects allocated in the main thread. 
\item Third, we employ huge page support for large objects and small objects that are projected to be allocated extensively. 
\item In the end, this paper also includes a large number of careful designs to reduce the performance and memory overhead. 
\end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:overview} introduces some background, such as the NUMA architecture and the OS support. Section~\ref{sec:implement} focuses on the design and implementation of \NM{}. After that, Section~\ref{sec:evaluation} describes its experimentation evaluation, and Section~\ref{sec:limit} discusses the limitation of \NM{}. In the end, Section\ref{sec:related} discusses some relevant related work, and then Section~\ref{sec:conclusion} concludes. 

\begin{comment}

eve have evaluated it on a range of benchmarks and real applications on two different NUMA architecture. The evaluation showed that \NA{} could significant improve the performance by up to 2X, with the average of 10\% performance. The evaluation also show that \NA{} has a bigger performance improvement on  


 
How to design the new allocator? This is the new one. 

We will try to achieve the following target. 

(1) The performance will be as efficient as possible. 
(2) It is still 
For information-computable, we will achieve the following targets:
We should still use the address to infer the following information, such as the size of the object and the shadow memory placement.

We don't support many bags for the same size class, just one big bag for each size class of each heap. Why it is necessary? 

We would like to reduce the memory blowup as much as possible, where the memory will be returned back to the current thread's heap. 

In order to support that, maybe we could have a big heap for the same size class, but different threads will start from different placement. 

Is it good for the NUMA support in the future? For NUMA support, it is great if we can always return the memory back to its original 


Virtual address will be divided into multiple nodes. 

Then inside each step, we will divide a sub-heap into multiple mini-heaps, where each mini-heap will support one thread. 


Virtual address will be divided into multiple nodes. 

Then inside each per-node heap, we will further divide it into multiple mini-heaps, where each mini-heap will support one thread in order to reduce the possible contention. 

For each per-thread heap, we will have two free-lists for each size class. The first one will be used for the current thread, without the use of lock at all. The second one will be utilized for returning memory from other threads.  

RTDSCP is a way to know where a thread is executed on. 

%https://software.intel.com/en-us/forums/intel-isa-extensions/topic/280440

\todo{Should we put the metadata into the corresponding node? We will minimize the lock uses for each node, since that will invoke unnecessary remote accesses as well. }


% From Score: SCOREs runtime system will include a memory manager instrumented to track object ownership with low overhead by leveraging existing thread-local allocation buffers and using per-processor memory pools. The runtime system will sample memory access patterns to detect when memory should be relocated to or initially placed in memory closer to a specific processor

What is the uniqueness of NUMA architecture? 

Remote accesses and imbalance? 
What are the big issues that could cause the performance issues on the NUMA architecture. 

Programs also have some inherent imbalance. For instance, main thread typically will prepare the data for all children threads. Therefore, many existing tools discover that the block-wised allocation is one way to reduce the imbalance~\cite{XuNuma, XXX}. 
Also, many applications are typically utilizes a producer-consumer architecture, where the objects allocated in one thread will be deallocated by another thread. This issue, if not handle correctly, will cause the ownership thrifting issues, which will cause that it is impossible to know the placement of an object after a while. Very quickly, it may cause a lot of unnecessary remote accesses, when using the existing NUMA allocator~\cite{}.  

Therefore, a good NUMA allocator should deal with these inherent imbalance of programs. Also, it should deal with the normal problems of NUMA hardware. It should 
	
\subsection{Novelty}

\begin{itemize}
\item We will design a node-aware memory allocator that could actually identify the memory placement by using the virtual address.
\item We will minimize the synchronization, since that will impose unnecessary remote accesses. 
\item We will minimize remote accesses by putting the metadata into multiple nodes. 
\item We may support multiple types of allocation, such as block-wise false sharing memory accesses, or node-balanced memory accesses, relying on the indication from user space. 	
\item We will balance the memory consumption over all nodes, in order to avoid the contention of memory controller~\cite{Majo:2011:MSP:1987816.1987832}.  At least, we could balance the memory consumption among all allocators. Ideally, it is better to balance the memory accesses. 
\item We may track the relationship of all objects. Then those objects will be allocated correspondingly. For instance, if the objects are allocated in the main thread, we assume that they will be accessed as a shared mode. Then we would like to allocate in the node-balanced areana initially. 

\end{itemize}

\end{comment}


\begin{comment}


\begin{enumerate}
\item Dividing each node's memory into two parts, one for small objects, and one for big objects. 
\item Making the main thread to use a dedicate memory region. Memory will be allocated in different nodes in an interleaved way. However, this method has some benefits and shortcomings. If the memory is deallocated immediately, then it is time-consuming to put it remotely, and will hurt the performance. Therefore, it is necessary to identify such cases, and do not allocate such objects in the interleaved way. 
\item Fixing the merge and split for big objects
\item Thinking about the integration of Ding Chen's theory. (Let Xin to do this part)
\item Thinking about using the inside of the object for holding the pointers for link list, which will be faster and remove some memory overhead unnecessarily. (Let Xin to do this part)
\item For producer-consumer threads, we will also track which objects are allocated and freed in different threads. For those objects, maybe they could utilize the same mechanism as the main thread. But we will do this in the future.
%\item Could we use page-spans instead to ensure the sharing? 
\item Intercepting mmap and use the mainthread's interleaved idea. 

\item We will use huge pages by default in order to improve the performance. But we will utilize small bags for small size classes, which we will learn from TcMalloc. That is, we don't want to use 1 Megabyte information. Instead, we may utilize the 4 pages as an unit. 

	
\end{enumerate}
	
\end{comment}

