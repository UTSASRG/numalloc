
\section{Introduction}
\label{sec:intro}

The Non-Uniform Memory Access (NUMA) architecture is a scalable hardware design for multi-core and multi-processor era. Compared to Uniform Memory Access (UMA) architecture, the NUMA architecture avoids the bottleneck of using one memory controller, where each node/processor can access its own memory controller concurrently. But the NUMA architecture imposes multiple system challenges for parallel applications, such as remote accesses, interconnect congestion, and node imbalance~\citep{Blagodurov:2011:CNC:2002181.2002182}. 
%For instance, the latency of remote accesses is typically double to that of local accesses. 
The memory allocator is one such component that could help solve these challenges.   

However, general-purpose memory allocators, such as dlmalloc~\citep{dlmalloc},  Hoard~\citep{Hoard}, TCMalloc~\citep{tcmalloc}, jemalloc~\citep{jemalloc}, SuperMalloc~\citep{supermalloc} and Scalloc~\citep{Scalloc}, were designed for symmetric multiprocessing machines without considering the heterogeneity of the underlying hardware~\citep{mimalloc}. As a result, they can not achieve good performance, based on existing work~\citep{tcmallocnew, yang2019jarena} and our evaluation. In 2008, Kaminski built the first NUMA-aware memory allocator on top of TCMalloc~\citep{tcmallocnew}, called as \TN{} in the remainder of this paper. After that, multiple allocators with NUMA support were developed~\citep{ kim2013node, yang2019jarena, mimalloc}, but they typically take a very similar design as \TN{} , as further discussed in Section~\ref{sec:related}. Therefore, only \TN{}'s design is further described here. 

%, but without considering interconnect congestion and node imbalance that may also significantly affect the performance. 
\TN{} inherits per-thread cache from TCMalloc, but adding a freelist array and a page-span for each NUMA node. Allocations will be satisfied in an order from a per-thread cache, per-node freelist, and then per-node page-span. Initially, the memory of each page-span will be mapped to the node so that physical memory is also allocated from the same node. However, TCMalloc-NUMA has multiple issues that may significantly affect its performance. First, it did not control deallocations of objects. A freed object is always placed into the deallocating thread's local buffer, a mechanism originated from TcMalloc~\citep{tcmalloc}. However, if such an object originates from a remote node initially, especially in the producer-consumer model, it obviously results in unnecessary remote accesses. Second, it did not control the placement of threads: a thread can be migrated to a remote node upon synchronizations or system calls, which not only forces the thread to access its stack remotely but also reloads all the data that are already in the cache of its original node. Further, after migration, all deallocated objects of the thread are later put into the freelist of the new node, causing unnecessary remote accesses. To the best of our knowledge, none of the existing NUMA allocators address these issues well.

\textbf{This paper proposes a novel memory allocator -- \NM{} -- that considers both hardware heterogeneity and allocation/deallocation attributes, with its fine-grained memory management}. \NM{} not only deals with hardware heterogeneity, but also employs different policies for objects with different attributes, such as share-ability, sizes, phase, and origin. We proposes multiple mechanisms that improve both load balance and locality, as detailed in the following.   

%Simply integrating thread binding, heap interleaving and huge page does not guarantee good performance. Instead, carefully design is required to properly coordinate the thread scheduling, memory object allocation/deallocation and huge page management to achieve better performance.

We propose an \textbf{origin-based memory management} ensuring that memory allocations are always satisfied from the local node of the requesting thread (called ``\textbf{local allocations}''). Note that the origin-based memory management is significantly different from the first-touch policy~\citep{lameter2013numa}. Under the first touch policy, a physical page is allocated from the node where a process first touches a virtual page. However, the first-touch policy does not handle freed objects, which could still introduce significant remote accesses in multi-threaded environment.  
Instead, \textit{the origin-based mechanism will track the origin of each object, and return a freed object based on its origin:} a freed object will be placed into the deallocating thread's local buffer (heap) \textbf{only if} it is originated from the same node that the current thread is running on; otherwise, it will be returned to its original node. \textit{Second, \NM{} introduces an origin-computable design that the physical origin can be simply computed within constant time}, as further described in Section~\ref{sec:overview}. This is important to reduce the overhead of checking the origin of each object, especially for applications with extensive deallocations. 
\textit{Third, it further includes an origin-based allocation that will perform the allocations based on the origin of a thread, which node the thread is running on, in order to ensure local allocations}.  \NM{}'s origin-based memory management ensures that  physical memory are always allocated from the local node on which a thread is running, unless otherwise stated as follows. 


We further propose two mechanisms to \textbf{reduce load imbalance among different NUMA nodes}, another major source of NUMA performance issues~\citep{Dashti:2013:TMH:2451116.2451157}. \textit{First, it introduces an interleaved heap that its physical pages are allocated from different physical nodes in an interleaved way, which reduces load imbalance for the initial node that the main thread is running on}. This is inspired by existing profiling tools that shared objects allocated in the initial thread are the most common source of performance degradation in the NUMA architecture~\citep{XULIU, MemProf}. Under the first-touch policy, all shared objects of the main thread will be allocated from the node of the main thread, while concurrent accesses from multiple children threads will make this node the performance bottleneck. Instead, potentially-shared objects of the main thread (and before creating children threads) will be allocated from the interleaved heap, which helps distribute concurrent accesses from children threads to all nodes. Note that objects from the interleaved heap will be always tracked in its own corresponding freelists. \textit{Second,  a node-balanced thread binding is further introduced to reduce load imbalance}. We argue that thread-binding is helpful for the NUMA architecture, since a cross-node migration will basically turn all previously-local accesses of a thread into remote ones, leading to the large performance degradation. However, the thread-binding itself is not new, which may introduce multiple issues if not handled properly. For instance, it cannot conflict with the OS-initiated scheduling. \NM{} binds a thread to a specific node, but not to a particular core, which still allows the OS-initiated inside-node migration. \NM{} obtains the relationship between cores and nodes via existing \texttt{NUMA} APIs. 
To reduce the imbalance, \NM{} binds threads to nodes in a round-robin way so that each node will have a similar number of threads, helping reduce the congestion of a particular memory controller~\citep{Blagodurov:2011:CNC:2002181.2002182}. Based on our knowledge, \NM{} is the first memory allocator that embeds the node-balanced thread binding inside. 
In the future, we plan to support user-controlled binding. 
%Due to the binding, \NM{} can  allocate the metadata of each thread (e.g., per-thread buffer) from the local node, which cannot be done without the binding. 

%in order to reduce load imbalance or interconnect congestion that occurs when a large amount of memory allocated in the initial thread are accessed extensively by children threads running on different nodes. 
%second, it may cause page-level false sharing, if a multi-page object is accessed concurrently by these threads, the most common NUMA performance detected by existing tools~\citep{XULIU, MemProf}. The underlying reason is that the first-touch policy makes objects allocated from the initial thread will be allocated from its corresponding node, as further described in Section~\ref{sec:ossupport}. The interleaved heap helps node balance, and alleviates page-level false sharing. 
 
\NM{} further introduces \textbf{selective huge pages} in order to exploit the benefit of huge pages. Huge pages will improve the performance if they are employed properly~\citep{DBLP:conf/asplos/MaasAIJMR20}, due to the reduction of TLB misses. However, the default transparent huge pages (THP) have been observed to hurt the performance on the NUMA architecture, due to unnecessary memory wastes and page-level false sharing~\citep{Gaud:2014:LPM:2643634.2643659, DBLP:conf/asplos/PanwarBG19, DBLP:conf/asplos/MaasAIJMR20}. \textit{\NM{} controls the type of objects that will be allocated from huge pages, in order to avoid the issues caused by huge pages}. In \NM{}, huge pages will be utilized only for big objects, as well as small objects with extensive allocations. This design is to reduce memory wastes. Based on our evaluation in Section~\ref{sec:hugepage}, \NM{} selective huge pages do not hurt the performance, compared to transparent huge pages.
 
\NM{} also solves some implementation challenges. Since \NM{} has many per-thread and per-node heaps, it is necessary to move freed objects between per-thread freelists and per-node freelists frequently. \NM{} designs an efficient mechanism to move objects between them, without traversing all objects in the freelists as TCMalloc~\citep{tcmalloc}. It allocates the metadata of per-thread and per-node heap on the local nodes in order to further reduce unnecessary remote accesses.  It also reduces memory consumption of huge pages by making multiple threads share the same memory block, overcoming one issue of Scalloc~\citep{Scalloc}. 

We have performed extensive evaluation on synthetic and real applications, and compared \NM{} with popular allocators, such as the default Linux allocator, TCMalloc~\citep{tcmalloc}, jemalloc~\citep{jemalloc}, Intel TBB~\citep{tbb}, Scalloc~\citep{Scalloc}, and mimalloc~\citep{mimalloc}. \NM{} achieves around 18.5\%  (GEOMEAN) speedup comparing to the default Linux allocator, which is also 15.8\% faster than the second-best one (e.g., TcMalloc). For the best case, \NM{} runs up to $6.4\times$ faster than the default allocator, and $4.6\times$ faster than the second-best one (TcMalloc). \NM{} is much more scalable than other allocators based on our evaluation. \NM{} is ready for practical employment, due to its high performance and good scalability. \\

\textbf{Overall, this paper makes the following contributions:} 

\begin{itemize}

\item It proposes a \textit{fine-grained memory management} method that will consider both hardware heterogeneity and different allocation/deallocation attributes in memory management operations.

\item It proposes an \textit{origin-based memory management} to ensure locality of all memory allocations. It also proposes \textit{the interleaved heap and node-balanced thread binding} to reduce node imbalance, and \textit{selective huge pages} to overcome the issues of transparent huge pages. 

\item It presents the development and implementation of \NM{}, which has a better performance and scalability than widely-used commercial allocators, such as TCMalloc, jemalloc, and Intel TBB. 

\end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:background} introduces the OS support for the NUMA architecture and common designs of memory allocators. Section~\ref{sec:implement} focuses on the design and implementation of \NM{}. After that, Section~\ref{sec:evaluation} describes its experimental evaluation, and Section~\ref{sec:limit} discusses the limitation of \NM{}. In the end, Section~\ref{sec:related} discusses some relevant work, and then Section~\ref{sec:conclusion} concludes. 