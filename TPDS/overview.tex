\section{Background}
\label{sec:overview}

%This section introduces some background that is necessary for \NM{}. 
This section discusses existing OS support for the NUMA architecture, and then discusses some common designs of memory allocators that are adopted by \NM{}.  

\begin{comment}

\todo{Cutting half for this section}

\subsection{NUMA Architecture}

\label{sec:numa}

Traditional computers are using the Uniform Memory Access (UMA) model that all CPU cores are sharing a single memory controller, where any core can access the memory with the same latency (uniformly). However, the UMA architecture cannot accommodate the hardware trend with the increasing number of cores, since all of them may compete for the same memory controller. Therefore, the performance bottleneck is the memory controller in many-core machines, since a task cannot proceed without getting its necessary data from the memory. 

The Non-Uniform Memory Access (NUMA) architecture is proposed to solve the scalability issue, due to its decentralized nature. Instead of making all cores waiting for the same memory controller, the NUMA architecture typically is installed with multiple memory controllers, where a group of CPU cores has its memory controller (called as a node). Due to multiple memory controllers, the contention for the memory controller could be largely reduced and therefore the scalability could be improved correspondingly. However, the NUMA architecture also has multiple sources of performance degradations~\cite{Blagodurov:2011:CNC:2002181.2002182}, including \textit{Cache Contention}, \textit{Node Imbalance}, \textit{Interconnect Congestion}, and \textit{Remote Accesses}. 

\paragraph{Cache Contention:} the NUMA architecture is prone to cache contention that multiple tasks may compete for the shared cache. Cache contention will introduce more serious performance issue if the data has to be loaded from a remote node. 
 
\paragraph{Node Imbalance:} When some memory controllers have much more memory accesses than others, it may cause the node imbalance issue. Therefore, some tasks may wait more time for memory accesses, thwarting the whole progress of a multithreaded application.  

\paragraph{Interconnect Congestion:} Interconnect congestion occurs if some tasks are placed in remote nodes that may use the inter-node interconnection to access their memory. 

\paragraph{Remote Accesses:} In NUMA architecture, local nodes can be accessed with less latency than remote accesses. Therefore, it is important to reduce remote accesses to improve the performance.\\


 Based on the study~\cite{Blagodurov:2011:CNC:2002181.2002182}, node imbalance and interconnect congestion may have a larger performance impact than cache contention and remote accesses. These performance issues cannot be solved by the hardware automatically. Software support is required to control the placement of tasks, physical pages, and objects to achieve the optimal performance for multithreaded applications.  

	
\end{comment}

\paragraph{NUMA Support Inside OS} Operating System already supports the NUMA architecture, especially on task scheduling and physical memory allocation. For the task scheduling support, the OS provides some system calls that allow users to bind a task to a specific node. For memory allocation, the OS also provides system calls (e.g., \texttt{mbind}) to change memory allocation policy for a range of memory or the whole process~\cite{lameter2013numa, diener2015locality}. However, they still require programmers to specify the policy explicitly, which cannot achieve the performance speedup automatically. \NM{} relies on these existing system calls to manage memory allocations explicitly, as further described in Section~\ref{sec:implement}.

\paragraph{Common Designs of Memory Allocators}
Memory allocators share some common designs. First, most allocators manage objects differently based on the size of objects. For big objects that are typically unusual, allocators may request objects from the OS directly and return them to the OS directly upon deallocation~\citep{Hoard}. On the other hand, small objects will be tracked in freelists based on size classes. The method of managing small objects can be further classified into multiple categories, such as sequential, BiBOP, and region-based allocators~\citep{DieHarder, Gay:1998:MME:277650.277748}. Region-based allocators that  all allocated objects within the same region are deallocated together do not belong to general-purpose allocators~\citep{Gay:1998:MME:277650.277748}. For sequential allocators, subsequent memory allocations are satisfied in a continuous memory block~\citep{Cling}. BiBOP-style allocators, which stands for ''Big Bag of Pages''~\citep{hanson1980portable}, utilize one or multiple continuous pages as a ``bag'' that hold objects of the same size class. Many performance-oriented allocators, such as TcMalloc~\citep{tcmalloc}, \texttt{jemalloc}~\citep{jemalloc}, Hoard~\citep{Hoard}, Scalloc~\citep{Scalloc}, and most of secure allocators, such as OpenBSD~\citep{openbsd} and DieHarder~\citep{DieHarder}, belong to this category. 

\NM{} borrows some of these existing mechanisms. First, it uses different mechanisms to manage small and big objects. Second, small objects are also managed by size classes using the BiBOP-style. \NA{} utilizes fine-grained size classes for small objects, such as 16 bytes apart for objects less than 128 bytes, and 32 bytes apart for objects between 128 bytes and 256 bytes, then power-of-2 sizes afterwards. Third, similar to existing work, such as Linux and TcMalloc~\cite{tcmalloc}, \NA{} utilizes different freelists to track freed objects, and uses the first word of freed objects to link different objects. But the difference of \NM{} is further described in Section~\ref{sec:implement}. 

%\NA{} only tracks the size information of each page, which helps reduce its memory overhead for the metadata. Third, \NA{} utilizes freelist to manage freed objects. Every freed object will be added into a corresponding freelist, and objects in the freelist will be allocated first in order to reduce possible cache misses. Further,  This mechanism helps to reduce the memory overhead, but is prone to memory vulnerabilities, such as buffer overflows and double-frees~\cite{DieHarder, Guarder}.     
 